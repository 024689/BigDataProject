[2024-01-29T14:11:15.189+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: manage_data_lake_dag.job_raw_to_formatted scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-29T14:11:15.194+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: manage_data_lake_dag.job_raw_to_formatted scheduled__2024-01-01T00:00:00+00:00 [queued]>
[2024-01-29T14:11:15.195+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 6
[2024-01-29T14:11:15.207+0000] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): job_raw_to_formatted> on 2024-01-01 00:00:00+00:00
[2024-01-29T14:11:15.211+0000] {standard_task_runner.py:60} INFO - Started process 41470 to run task
[2024-01-29T14:11:15.216+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'manage_data_lake_dag', 'job_raw_to_formatted', 'scheduled__2024-01-01T00:00:00+00:00', '--job-id', '119', '--raw', '--subdir', 'DAGS_FOLDER/data_lake_dag.py', '--cfg-path', '/tmp/tmp2k_2fch0']
[2024-01-29T14:11:15.219+0000] {standard_task_runner.py:88} INFO - Job 119: Subtask job_raw_to_formatted
[2024-01-29T14:11:15.258+0000] {task_command.py:423} INFO - Running <TaskInstance: manage_data_lake_dag.job_raw_to_formatted scheduled__2024-01-01T00:00:00+00:00 [running]> on host instance-1.europe-west9-a.c.data-lake-project-409321.internal
[2024-01-29T14:11:15.312+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='juniortemgoua0' AIRFLOW_CTX_DAG_ID='manage_data_lake_dag' AIRFLOW_CTX_TASK_ID='job_raw_to_formatted' AIRFLOW_CTX_EXECUTION_DATE='2024-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-01-01T00:00:00+00:00'
[2024-01-29T14:11:15.318+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2024-01-29T14:11:15.319+0000] {spark_submit.py:401} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark --class RawToFormatted --queue root.default --deploy-mode client /home/juniortemgoua0/DataLake/jobs/processes/scala/spark_process/target/scala-2.12/spark_job_2.12-0.1.0.jar gs://data-lake-buck
[2024-01-29T14:11:18.719+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:18 INFO SparkContext: Running Spark version 3.5.0
[2024-01-29T14:11:18.730+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:18 INFO SparkContext: OS info Linux, 5.15.0-1049-gcp, amd64
[2024-01-29T14:11:18.731+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:18 INFO SparkContext: Java version 17.0.9
[2024-01-29T14:11:18.896+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-01-29T14:11:19.084+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO ResourceUtils: ==============================================================
[2024-01-29T14:11:19.086+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-01-29T14:11:19.087+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO ResourceUtils: ==============================================================
[2024-01-29T14:11:19.088+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SparkContext: Submitted application: RawToFormatted
[2024-01-29T14:11:19.126+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-01-29T14:11:19.136+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO ResourceProfile: Limiting resource is cpu
[2024-01-29T14:11:19.137+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-01-29T14:11:19.225+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SecurityManager: Changing view acls to: juniortemgoua0
[2024-01-29T14:11:19.227+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SecurityManager: Changing modify acls to: juniortemgoua0
[2024-01-29T14:11:19.227+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SecurityManager: Changing view acls groups to:
[2024-01-29T14:11:19.228+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SecurityManager: Changing modify acls groups to:
[2024-01-29T14:11:19.229+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: juniortemgoua0; groups with view permissions: EMPTY; users with modify permissions: juniortemgoua0; groups with modify permissions: EMPTY
[2024-01-29T14:11:19.732+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO Utils: Successfully started service 'sparkDriver' on port 46395.
[2024-01-29T14:11:19.818+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SparkEnv: Registering MapOutputTracker
[2024-01-29T14:11:19.877+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SparkEnv: Registering BlockManagerMaster
[2024-01-29T14:11:19.921+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-01-29T14:11:19.922+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-01-29T14:11:19.932+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-01-29T14:11:19.983+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3a9cdb44-1cb3-4d67-aed9-11ce8602115c
[2024-01-29T14:11:20.026+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-01-29T14:11:20.061+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-01-29T14:11:20.328+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-01-29T14:11:20.464+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-01-29T14:11:20.565+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO SparkContext: Added JAR file:/home/juniortemgoua0/DataLake/jobs/processes/scala/spark_process/target/scala-2.12/spark_job_2.12-0.1.0.jar at spark://instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395/jars/spark_job_2.12-0.1.0.jar with timestamp 1706537478698
[2024-01-29T14:11:20.719+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Executor: Starting executor ID driver on host instance-1.europe-west9-a.c.data-lake-project-409321.internal
[2024-01-29T14:11:20.719+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Executor: OS info Linux, 5.15.0-1049-gcp, amd64
[2024-01-29T14:11:20.721+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Executor: Java version 17.0.9
[2024-01-29T14:11:20.782+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-01-29T14:11:20.792+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3d19d85 for default.
[2024-01-29T14:11:20.822+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Executor: Fetching spark://instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395/jars/spark_job_2.12-0.1.0.jar with timestamp 1706537478698
[2024-01-29T14:11:20.923+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO TransportClientFactory: Successfully created connection to instance-1.europe-west9-a.c.data-lake-project-409321.internal/10.200.0.5:46395 after 52 ms (0 ms spent in bootstraps)
[2024-01-29T14:11:20.943+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:20 INFO Utils: Fetching spark://instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395/jars/spark_job_2.12-0.1.0.jar to /tmp/spark-17e43efd-3708-4676-820c-ec11a4965aab/userFiles-9e32891f-7e49-4eda-bd04-46243d05bec6/fetchFileTemp9420993482035295700.tmp
[2024-01-29T14:11:21.016+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO Executor: Adding file:/tmp/spark-17e43efd-3708-4676-820c-ec11a4965aab/userFiles-9e32891f-7e49-4eda-bd04-46243d05bec6/spark_job_2.12-0.1.0.jar to class loader default
[2024-01-29T14:11:21.041+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33393.
[2024-01-29T14:11:21.042+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO NettyBlockTransferService: Server created on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393
[2024-01-29T14:11:21.045+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-01-29T14:11:21.060+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:11:21.064+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO BlockManagerMasterEndpoint: Registering block manager instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 with 434.4 MiB RAM, BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:11:21.066+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:11:21.069+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:11:22.366+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-01-29T14:11:22.383+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:22 INFO SharedState: Warehouse path is 'file:/home/juniortemgoua0/DataLake/spark-warehouse'.
[2024-01-29T14:11:25.606+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:25 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=831; previousMaxLatencyMs=0; operationCount=1; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/*.json
[2024-01-29T14:11:25.798+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:25 INFO GhfsStorageStatistics: Detected potential high latency for operation op_glob_status. latencyMs=172; previousMaxLatencyMs=0; operationCount=1; context=path=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/*.json; pattern=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase$$Lambda$978/0x00007f4a3c616d50@54e1d3dc
[2024-01-29T14:11:27.802+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:27 INFO InMemoryFileIndex: It took 1952 ms to list leaf files for 13 paths.
[2024-01-29T14:11:28.038+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2024-01-29T14:11:28.135+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 434.2 MiB)
[2024-01-29T14:11:28.138+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 35.1 KiB, free: 434.4 MiB)
[2024-01-29T14:11:28.144+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:28 INFO SparkContext: Created broadcast 0 from json at RawToFormatted.scala:34
[2024-01-29T14:11:29.171+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:29 INFO GhfsStorageStatistics: Detected potential high latency for operation op_glob_status. latencyMs=289; previousMaxLatencyMs=172; operationCount=4; context=path=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CL.json; pattern=org.apache.hadoop.mapreduce.lib.input.FileInputFormat$MultiPathFilter@627d5f1
[2024-01-29T14:11:30.553+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:30 INFO FileInputFormat: Total input files to process : 13
[2024-01-29T14:11:31.535+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:31 INFO GhfsStorageStatistics: Detected potential high latency for operation op_glob_status. latencyMs=290; previousMaxLatencyMs=289; operationCount=20; context=path=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/EC.json; pattern=org.apache.hadoop.mapreduce.lib.input.FileInputFormat$MultiPathFilter@361d8567
[2024-01-29T14:11:32.632+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO FileInputFormat: Total input files to process : 13
[2024-01-29T14:11:32.682+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO SparkContext: Starting job: json at RawToFormatted.scala:34
[2024-01-29T14:11:32.707+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO DAGScheduler: Got job 0 (json at RawToFormatted.scala:34) with 1 output partitions
[2024-01-29T14:11:32.708+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO DAGScheduler: Final stage: ResultStage 0 (json at RawToFormatted.scala:34)
[2024-01-29T14:11:32.709+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO DAGScheduler: Parents of final stage: List()
[2024-01-29T14:11:32.711+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:11:32.718+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at json at RawToFormatted.scala:34), which has no missing parents
[2024-01-29T14:11:32.821+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.2 KiB, free 434.2 MiB)
[2024-01-29T14:11:32.825+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 434.2 MiB)
[2024-01-29T14:11:32.825+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 4.9 KiB, free: 434.4 MiB)
[2024-01-29T14:11:32.826+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:11:32.859+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at RawToFormatted.scala:34) (first 15 tasks are for partitions Vector(0))
[2024-01-29T14:11:32.860+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-01-29T14:11:32.926+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, PROCESS_LOCAL, 9052 bytes)
[2024-01-29T14:11:32.953+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-01-29T14:11:33.074+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:33 INFO BinaryFileRDD: Input split: Paths:/raw/football_data/competitions/2024-01-29/BL1.json:0+20528,/raw/football_data/competitions/2024-01-29/BSA.json:0+3389,/raw/football_data/competitions/2024-01-29/CL.json:0+8210,/raw/football_data/competitions/2024-01-29/CLI.json:0+1055,/raw/football_data/competitions/2024-01-29/DED.json:0+11649,/raw/football_data/competitions/2024-01-29/EC.json:0+9263,/raw/football_data/competitions/2024-01-29/ELC.json:0+2334,/raw/football_data/competitions/2024-01-29/FL1.json:0+12632,/raw/football_data/competitions/2024-01-29/PD.json:0+15902,/raw/football_data/competitions/2024-01-29/PL.json:0+31329,/raw/football_data/competitions/2024-01-29/PPL.json:0+12814,/raw/football_data/competitions/2024-01-29/SA.json:0+15666,/raw/football_data/competitions/2024-01-29/WC.json:0+13533
[2024-01-29T14:11:33.310+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:33 INFO GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=154; previousMaxLatencyMs=0; operationCount=1; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BL1.json
[2024-01-29T14:11:33.551+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:33 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_read_operations. latencyMs=194; previousMaxLatencyMs=0; operationCount=0; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BL1.json
[2024-01-29T14:11:36.849+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3087 bytes result sent to driver
[2024-01-29T14:11:36.866+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3960 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (1/1)
[2024-01-29T14:11:36.872+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-01-29T14:11:36.877+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:36 INFO DAGScheduler: ResultStage 0 (json at RawToFormatted.scala:34) finished in 4.121 s
[2024-01-29T14:11:36.885+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-29T14:11:36.886+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-01-29T14:11:36.888+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:36 INFO DAGScheduler: Job 0 finished: json at RawToFormatted.scala:34, took 4.205350 s
[2024-01-29T14:11:37.046+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 4.9 KiB, free: 434.4 MiB)
[2024-01-29T14:11:37.915+0000] {spark_submit.py:571} INFO - root
[2024-01-29T14:11:37.915+0000] {spark_submit.py:571} INFO - |-- area: struct (nullable = true)
[2024-01-29T14:11:37.915+0000] {spark_submit.py:571} INFO - |    |-- code: string (nullable = true)
[2024-01-29T14:11:37.915+0000] {spark_submit.py:571} INFO - |    |-- flag: string (nullable = true)
[2024-01-29T14:11:37.915+0000] {spark_submit.py:571} INFO - |    |-- id: long (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |    |-- name: string (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |-- code: string (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |-- currentSeason: struct (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |    |-- currentMatchday: long (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |    |-- endDate: string (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |    |-- id: long (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |    |-- startDate: string (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |    |-- winner: struct (nullable = true)
[2024-01-29T14:11:37.916+0000] {spark_submit.py:571} INFO - |    |    |-- address: string (nullable = true)
[2024-01-29T14:11:37.917+0000] {spark_submit.py:571} INFO - |    |    |-- clubColors: string (nullable = true)
[2024-01-29T14:11:37.917+0000] {spark_submit.py:571} INFO - |    |    |-- crest: string (nullable = true)
[2024-01-29T14:11:37.917+0000] {spark_submit.py:571} INFO - |    |    |-- founded: long (nullable = true)
[2024-01-29T14:11:37.917+0000] {spark_submit.py:571} INFO - |    |    |-- id: long (nullable = true)
[2024-01-29T14:11:37.917+0000] {spark_submit.py:571} INFO - |    |    |-- lastUpdated: string (nullable = true)
[2024-01-29T14:11:37.917+0000] {spark_submit.py:571} INFO - |    |    |-- name: string (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |    |    |-- shortName: string (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |    |    |-- tla: string (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |    |    |-- venue: string (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |    |    |-- website: string (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |-- emblem: string (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |-- id: long (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |-- lastUpdated: string (nullable = true)
[2024-01-29T14:11:37.918+0000] {spark_submit.py:571} INFO - |-- name: string (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |-- seasons: array (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |-- element: struct (containsNull = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |    |-- currentMatchday: long (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |    |-- endDate: string (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |    |-- id: long (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |    |-- startDate: string (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |    |-- winner: struct (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |    |    |-- address: string (nullable = true)
[2024-01-29T14:11:37.919+0000] {spark_submit.py:571} INFO - |    |    |    |-- clubColors: string (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- crest: string (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- founded: long (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- id: long (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- lastUpdated: string (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- name: string (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- shortName: string (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- tla: string (nullable = true)
[2024-01-29T14:11:37.920+0000] {spark_submit.py:571} INFO - |    |    |    |-- venue: string (nullable = true)
[2024-01-29T14:11:37.921+0000] {spark_submit.py:571} INFO - |    |    |    |-- website: string (nullable = true)
[2024-01-29T14:11:37.921+0000] {spark_submit.py:571} INFO - |-- type: string (nullable = true)
[2024-01-29T14:11:37.921+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:38.730+0000] {spark_submit.py:571} INFO - root
[2024-01-29T14:11:38.731+0000] {spark_submit.py:571} INFO - |-- competitionId: long (nullable = true)
[2024-01-29T14:11:38.731+0000] {spark_submit.py:571} INFO - |-- competitionCode: string (nullable = true)
[2024-01-29T14:11:38.731+0000] {spark_submit.py:571} INFO - |-- id: long (nullable = true)
[2024-01-29T14:11:38.731+0000] {spark_submit.py:571} INFO - |-- endDate: string (nullable = true)
[2024-01-29T14:11:38.731+0000] {spark_submit.py:571} INFO - |-- startDate: string (nullable = true)
[2024-01-29T14:11:38.732+0000] {spark_submit.py:571} INFO - |-- winnerTeamId: long (nullable = true)
[2024-01-29T14:11:38.732+0000] {spark_submit.py:571} INFO - |-- currentMatchDay: long (nullable = true)
[2024-01-29T14:11:38.732+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:39.461+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(seasons)
[2024-01-29T14:11:39.464+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:39 INFO FileSourceStrategy: Post-Scan Filters: (size(seasons#7, true) > 0),isnotnull(seasons#7)
[2024-01-29T14:11:40.191+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO CodeGenerator: Code generated in 447.026294 ms
[2024-01-29T14:11:40.200+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)
[2024-01-29T14:11:40.212+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.9 MiB)
[2024-01-29T14:11:40.214+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 34.8 KiB, free: 434.3 MiB)
[2024-01-29T14:11:40.215+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO SparkContext: Created broadcast 2 from show at RawToFormatted.scala:62
[2024-01-29T14:11:40.233+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 27342128 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-29T14:11:40.281+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO SparkContext: Starting job: show at RawToFormatted.scala:62
[2024-01-29T14:11:40.283+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO DAGScheduler: Got job 1 (show at RawToFormatted.scala:62) with 1 output partitions
[2024-01-29T14:11:40.283+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO DAGScheduler: Final stage: ResultStage 1 (show at RawToFormatted.scala:62)
[2024-01-29T14:11:40.283+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO DAGScheduler: Parents of final stage: List()
[2024-01-29T14:11:40.284+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:11:40.284+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at show at RawToFormatted.scala:62), which has no missing parents
[2024-01-29T14:11:40.341+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.2 KiB, free 433.9 MiB)
[2024-01-29T14:11:40.344+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.0 KiB, free 433.9 MiB)
[2024-01-29T14:11:40.345+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 10.0 KiB, free: 434.3 MiB)
[2024-01-29T14:11:40.345+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:11:40.346+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at RawToFormatted.scala:62) (first 15 tasks are for partitions Vector(0))
[2024-01-29T14:11:40.346+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-01-29T14:11:40.355+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, PROCESS_LOCAL, 9238 bytes)
[2024-01-29T14:11:40.355+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-01-29T14:11:40.573+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO CodeGenerator: Code generated in 120.800199 ms
[2024-01-29T14:11:40.584+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PL.json, range: 0-31329, partition values: [empty row]
[2024-01-29T14:11:40.628+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 35.1 KiB, free: 434.4 MiB)
[2024-01-29T14:11:40.685+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO CodeGenerator: Code generated in 75.097002 ms
[2024-01-29T14:11:40.734+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:40 INFO CodeGenerator: Code generated in 11.104826 ms
[2024-01-29T14:11:41.018+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2408 bytes result sent to driver
[2024-01-29T14:11:41.018+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 667 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (1/1)
[2024-01-29T14:11:41.019+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:41 INFO DAGScheduler: ResultStage 1 (show at RawToFormatted.scala:62) finished in 0.733 s
[2024-01-29T14:11:41.020+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:41 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-29T14:11:41.024+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-01-29T14:11:41.025+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-01-29T14:11:41.025+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:41 INFO DAGScheduler: Job 1 finished: show at RawToFormatted.scala:62, took 0.743306 s
[2024-01-29T14:11:42.472+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:42 INFO CodeGenerator: Code generated in 18.771869 ms
[2024-01-29T14:11:42.494+0000] {spark_submit.py:571} INFO - +-------------+---------------+----+----------+----------+------------+---------------+
[2024-01-29T14:11:42.495+0000] {spark_submit.py:571} INFO - |competitionId|competitionCode|  id|   endDate| startDate|winnerTeamId|currentMatchDay|
[2024-01-29T14:11:42.495+0000] {spark_submit.py:571} INFO - +-------------+---------------+----+----------+----------+------------+---------------+
[2024-01-29T14:11:42.495+0000] {spark_submit.py:571} INFO - |         2021|             PL|1564|2024-05-19|2023-08-11|        NULL|             22|
[2024-01-29T14:11:42.495+0000] {spark_submit.py:571} INFO - |         2021|             PL|1490|2023-05-28|2022-08-05|          65|             38|
[2024-01-29T14:11:42.496+0000] {spark_submit.py:571} INFO - |         2021|             PL| 733|2022-05-22|2021-08-13|          65|             38|
[2024-01-29T14:11:42.496+0000] {spark_submit.py:571} INFO - |         2021|             PL| 619|2021-05-23|2020-09-12|          65|             38|
[2024-01-29T14:11:42.496+0000] {spark_submit.py:571} INFO - |         2021|             PL| 468|2020-07-26|2019-08-09|          64|             38|
[2024-01-29T14:11:42.496+0000] {spark_submit.py:571} INFO - |         2021|             PL| 151|2019-05-12|2018-08-10|          65|             38|
[2024-01-29T14:11:42.496+0000] {spark_submit.py:571} INFO - |         2021|             PL|  23|2018-05-13|2017-08-11|          65|             38|
[2024-01-29T14:11:42.496+0000] {spark_submit.py:571} INFO - |         2021|             PL| 256|2017-05-21|2016-08-13|          61|           NULL|
[2024-01-29T14:11:42.496+0000] {spark_submit.py:571} INFO - |         2021|             PL| 257|2016-05-17|2015-08-08|         338|           NULL|
[2024-01-29T14:11:42.497+0000] {spark_submit.py:571} INFO - |         2021|             PL| 258|2015-05-24|2014-08-16|          61|           NULL|
[2024-01-29T14:11:42.497+0000] {spark_submit.py:571} INFO - |         2021|             PL| 259|2014-05-11|2013-08-17|          65|           NULL|
[2024-01-29T14:11:42.497+0000] {spark_submit.py:571} INFO - |         2021|             PL| 260|2013-05-19|2012-08-18|          66|           NULL|
[2024-01-29T14:11:42.497+0000] {spark_submit.py:571} INFO - |         2021|             PL| 261|2012-05-13|2011-08-13|          65|           NULL|
[2024-01-29T14:11:42.497+0000] {spark_submit.py:571} INFO - |         2021|             PL| 262|2011-05-22|2010-08-14|          66|           NULL|
[2024-01-29T14:11:42.497+0000] {spark_submit.py:571} INFO - |         2021|             PL| 263|2010-05-09|2009-08-15|          61|           NULL|
[2024-01-29T14:11:42.497+0000] {spark_submit.py:571} INFO - |         2021|             PL| 264|2009-05-24|2008-08-16|          66|           NULL|
[2024-01-29T14:11:42.498+0000] {spark_submit.py:571} INFO - |         2021|             PL| 265|2008-05-11|2007-08-11|          66|           NULL|
[2024-01-29T14:11:42.498+0000] {spark_submit.py:571} INFO - |         2021|             PL| 266|2007-05-13|2006-08-19|          66|           NULL|
[2024-01-29T14:11:42.498+0000] {spark_submit.py:571} INFO - |         2021|             PL| 267|2006-05-07|2005-08-13|          61|           NULL|
[2024-01-29T14:11:42.498+0000] {spark_submit.py:571} INFO - |         2021|             PL| 268|2005-05-15|2004-08-14|          61|           NULL|
[2024-01-29T14:11:42.498+0000] {spark_submit.py:571} INFO - +-------------+---------------+----+----------+----------+------------+---------------+
[2024-01-29T14:11:42.498+0000] {spark_submit.py:571} INFO - only showing top 20 rows
[2024-01-29T14:11:42.498+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:42.595+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 10.0 KiB, free: 434.4 MiB)
[2024-01-29T14:11:42.623+0000] {spark_submit.py:571} INFO - root
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- code: string (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- emblem: string (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- id: long (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- lastUpdated: string (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- name: string (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- type: string (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- areaCode: string (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- areaFlag: string (nullable = true)
[2024-01-29T14:11:42.624+0000] {spark_submit.py:571} INFO - |-- areaId: long (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - |-- areaName: string (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - |-- competitionId: long (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - |-- competitionCode: string (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - |-- currentSeasonId: long (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - |-- currentMatchDay: long (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - |-- seasonId: long (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - |-- seasonWinnerId: long (nullable = true)
[2024-01-29T14:11:42.625+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:42.833+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(seasons)
[2024-01-29T14:11:42.834+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:42 INFO FileSourceStrategy: Post-Scan Filters: (size(seasons#7, true) > 0),isnotnull(seasons#7)
[2024-01-29T14:11:42.925+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2024-01-29T14:11:43.187+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO CodeGenerator: Code generated in 129.861436 ms
[2024-01-29T14:11:43.195+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)
[2024-01-29T14:11:43.207+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.9 MiB)
[2024-01-29T14:11:43.209+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 34.8 KiB, free: 434.3 MiB)
[2024-01-29T14:11:43.210+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO SparkContext: Created broadcast 4 from show at RawToFormatted.scala:71
[2024-01-29T14:11:43.216+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 27342128 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-29T14:11:43.339+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO DAGScheduler: Registering RDD 11 (show at RawToFormatted.scala:71) as input to shuffle 0
[2024-01-29T14:11:43.343+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO DAGScheduler: Got map stage job 2 (show at RawToFormatted.scala:71) with 2 output partitions
[2024-01-29T14:11:43.344+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (show at RawToFormatted.scala:71)
[2024-01-29T14:11:43.344+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO DAGScheduler: Parents of final stage: List()
[2024-01-29T14:11:43.345+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:11:43.348+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at show at RawToFormatted.scala:71), which has no missing parents
[2024-01-29T14:11:43.377+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 66.1 KiB, free 433.9 MiB)
[2024-01-29T14:11:43.379+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 24.5 KiB, free 433.9 MiB)
[2024-01-29T14:11:43.380+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 24.5 KiB, free: 434.3 MiB)
[2024-01-29T14:11:43.382+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:11:43.384+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at show at RawToFormatted.scala:71) (first 15 tasks are for partitions Vector(0, 1))
[2024-01-29T14:11:43.385+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2024-01-29T14:11:43.388+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, PROCESS_LOCAL, 9227 bytes)
[2024-01-29T14:11:43.389+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 1, PROCESS_LOCAL, 9102 bytes)
[2024-01-29T14:11:43.389+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-01-29T14:11:43.412+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
[2024-01-29T14:11:43.555+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO CodeGenerator: Code generated in 92.342809 ms
[2024-01-29T14:11:43.586+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO CodeGenerator: Code generated in 19.914265 ms
[2024-01-29T14:11:43.620+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO CodeGenerator: Code generated in 10.781908 ms
[2024-01-29T14:11:43.674+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/DED.json, range: 0-11649, partition values: [empty row]
[2024-01-29T14:11:43.674+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PL.json, range: 0-31329, partition values: [empty row]
[2024-01-29T14:11:43.728+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO CodeGenerator: Code generated in 33.295681 ms
[2024-01-29T14:11:43.738+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO CodeGenerator: Code generated in 7.200086 ms
[2024-01-29T14:11:43.965+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:43 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/EC.json, range: 0-9263, partition values: [empty row]
[2024-01-29T14:11:44.001+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BL1.json, range: 0-20528, partition values: [empty row]
[2024-01-29T14:11:44.222+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PD.json, range: 0-15902, partition values: [empty row]
[2024-01-29T14:11:44.251+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=273; previousMaxLatencyMs=154; operationCount=19; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/EC.json
[2024-01-29T14:11:44.363+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CL.json, range: 0-8210, partition values: [empty row]
[2024-01-29T14:11:44.436+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/SA.json, range: 0-15666, partition values: [empty row]
[2024-01-29T14:11:44.656+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/WC.json, range: 0-13533, partition values: [empty row]
[2024-01-29T14:11:44.751+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_read_operations. latencyMs=279; previousMaxLatencyMs=194; operationCount=45; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CL.json
[2024-01-29T14:11:44.753+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BSA.json, range: 0-3389, partition values: [empty row]
[2024-01-29T14:11:44.882+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PPL.json, range: 0-12814, partition values: [empty row]
[2024-01-29T14:11:44.967+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:44 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/ELC.json, range: 0-2334, partition values: [empty row]
[2024-01-29T14:11:45.094+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/FL1.json, range: 0-12632, partition values: [empty row]
[2024-01-29T14:11:45.245+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=275; previousMaxLatencyMs=273; operationCount=26; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/ELC.json
[2024-01-29T14:11:45.357+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CLI.json, range: 0-1055, partition values: [empty row]
[2024-01-29T14:11:45.402+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO CodeGenerator: Code generated in 47.076892 ms
[2024-01-29T14:11:45.481+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO CodeGenerator: Code generated in 46.064352 ms
[2024-01-29T14:11:45.493+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO CodeGenerator: Code generated in 6.838623 ms
[2024-01-29T14:11:45.594+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO CodeGenerator: Code generated in 63.393059 ms
[2024-01-29T14:11:45.640+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO CodeGenerator: Code generated in 29.492724 ms
[2024-01-29T14:11:45.845+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2779 bytes result sent to driver
[2024-01-29T14:11:45.859+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2472 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (1/2)
[2024-01-29T14:11:45.927+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 2779 bytes result sent to driver
[2024-01-29T14:11:45.930+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 2542 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (2/2)
[2024-01-29T14:11:45.934+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO DAGScheduler: ShuffleMapStage 2 (show at RawToFormatted.scala:71) finished in 2.581 s
[2024-01-29T14:11:45.934+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO DAGScheduler: looking for newly runnable stages
[2024-01-29T14:11:45.934+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO DAGScheduler: running: Set()
[2024-01-29T14:11:45.934+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO DAGScheduler: waiting: Set()
[2024-01-29T14:11:45.935+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO DAGScheduler: failed: Set()
[2024-01-29T14:11:45.939+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-01-29T14:11:45.974+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:45 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-01-29T14:11:46.037+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 12.843047 ms
[2024-01-29T14:11:46.106+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO SparkContext: Starting job: show at RawToFormatted.scala:71
[2024-01-29T14:11:46.110+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Got job 3 (show at RawToFormatted.scala:71) with 1 output partitions
[2024-01-29T14:11:46.110+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Final stage: ResultStage 4 (show at RawToFormatted.scala:71)
[2024-01-29T14:11:46.111+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2024-01-29T14:11:46.112+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:11:46.112+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at show at RawToFormatted.scala:71), which has no missing parents
[2024-01-29T14:11:46.141+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 63.8 KiB, free 433.8 MiB)
[2024-01-29T14:11:46.155+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.1 KiB, free 433.8 MiB)
[2024-01-29T14:11:46.156+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 25.1 KiB, free: 434.3 MiB)
[2024-01-29T14:11:46.156+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:11:46.159+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at show at RawToFormatted.scala:71) (first 15 tasks are for partitions Vector(0))
[2024-01-29T14:11:46.159+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-01-29T14:11:46.163+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 24.5 KiB, free: 434.3 MiB)
[2024-01-29T14:11:46.168+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, NODE_LOCAL, 7847 bytes)
[2024-01-29T14:11:46.170+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2024-01-29T14:11:46.238+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO ShuffleBlockFetcherIterator: Getting 2 (3.9 KiB) non-empty blocks including 2 (3.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-01-29T14:11:46.241+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2024-01-29T14:11:46.265+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 19.110437 ms
[2024-01-29T14:11:46.277+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 8.942565 ms
[2024-01-29T14:11:46.290+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 7.829077 ms
[2024-01-29T14:11:46.449+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 84.34166 ms
[2024-01-29T14:11:46.462+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 8.24577 ms
[2024-01-29T14:11:46.495+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 22.23293 ms
[2024-01-29T14:11:46.538+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 29.273065 ms
[2024-01-29T14:11:46.580+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 7045 bytes result sent to driver
[2024-01-29T14:11:46.582+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 417 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (1/1)
[2024-01-29T14:11:46.582+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-01-29T14:11:46.588+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: ResultStage 4 (show at RawToFormatted.scala:71) finished in 0.450 s
[2024-01-29T14:11:46.588+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-29T14:11:46.589+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-01-29T14:11:46.589+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO DAGScheduler: Job 3 finished: show at RawToFormatted.scala:71, took 0.479821 s
[2024-01-29T14:11:46.642+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO CodeGenerator: Code generated in 42.812245 ms
[2024-01-29T14:11:46.660+0000] {spark_submit.py:571} INFO - +----+--------------------+----+--------------------+--------------------+------+--------+--------------------+------+-------------+-------------+---------------+---------------+---------------+--------+--------------+
[2024-01-29T14:11:46.660+0000] {spark_submit.py:571} INFO - |code|              emblem|  id|         lastUpdated|                name|  type|areaCode|            areaFlag|areaId|     areaName|competitionId|competitionCode|currentSeasonId|currentMatchDay|seasonId|seasonWinnerId|
[2024-01-29T14:11:46.660+0000] {spark_submit.py:571} INFO - +----+--------------------+----+--------------------+--------------------+------+--------+--------------------+------+-------------+-------------+---------------+---------------+---------------+--------+--------------+
[2024-01-29T14:11:46.660+0000] {spark_submit.py:571} INFO - |  WC|https://crests.fo...|2000|2022-05-09T19:45:29Z|      FIFA World Cup|   CUP|     INT|                NULL|  2267|        World|         2000|             WC|           1382|              8|    1382|           762|
[2024-01-29T14:11:46.661+0000] {spark_submit.py:571} INFO - |  CL|https://crests.fo...|2001|2022-03-20T09:20:44Z|UEFA Champions Le...|   CUP|     EUR|https://crests.fo...|  2077|       Europe|         2001|             CL|           1630|              6|    1630|          NULL|
[2024-01-29T14:11:46.661+0000] {spark_submit.py:571} INFO - | BL1|https://crests.fo...|2002|2022-03-20T08:52:53Z|          Bundesliga|LEAGUE|     DEU|https://crests.fo...|  2088|      Germany|         2002|            BL1|           1592|             19|    1592|          NULL|
[2024-01-29T14:11:46.661+0000] {spark_submit.py:571} INFO - | DED|https://crests.fo...|2003|2022-03-20T09:19:27Z|          Eredivisie|LEAGUE|     NLD|https://crests.fo...|  2163|  Netherlands|         2003|            DED|           1590|             19|    1590|          NULL|
[2024-01-29T14:11:46.661+0000] {spark_submit.py:571} INFO - | BSA|https://crests.fo...|2013|2021-07-20T18:42:17Z|Campeonato Brasil...|LEAGUE|     BRA|https://crests.fo...|  2032|       Brazil|         2013|            BSA|           1557|             37|    1557|          NULL|
[2024-01-29T14:11:46.661+0000] {spark_submit.py:571} INFO - |  PD|https://crests.fo...|2014|2022-03-20T09:20:08Z|    Primera Division|LEAGUE|     ESP|https://crests.fo...|  2224|        Spain|         2014|             PD|           1577|             22|    1577|          NULL|
[2024-01-29T14:11:46.661+0000] {spark_submit.py:571} INFO - | FL1|https://crests.fo...|2015|2022-03-20T09:30:02Z|             Ligue 1|LEAGUE|     FRA|https://crests.fo...|  2081|       France|         2015|            FL1|           1595|             19|    1595|          NULL|
[2024-01-29T14:11:46.661+0000] {spark_submit.py:571} INFO - | ELC|https://crests.fo...|2016|2022-03-20T09:31:30Z|        Championship|LEAGUE|     ENG|https://crests.fo...|  2072|      England|         2016|            ELC|           1573|             29|    1573|          NULL|
[2024-01-29T14:11:46.662+0000] {spark_submit.py:571} INFO - | PPL|https://crests.fo...|2017|2022-03-20T09:34:09Z|       Primeira Liga|LEAGUE|     POR|https://crests.fo...|  2187|     Portugal|         2017|            PPL|           1603|             19|    1603|          NULL|
[2024-01-29T14:11:46.662+0000] {spark_submit.py:571} INFO - |  EC|https://crests.fo...|2018|2021-07-20T18:34:04Z|European Champion...|   CUP|     EUR|https://crests.fo...|  2077|       Europe|         2018|             EC|           1537|              1|    1537|          NULL|
[2024-01-29T14:11:46.662+0000] {spark_submit.py:571} INFO - |  SA|https://crests.fo...|2019|2022-03-20T09:16:43Z|             Serie A|LEAGUE|     ITA|https://crests.fo...|  2114|        Italy|         2019|             SA|           1600|             22|    1600|          NULL|
[2024-01-29T14:11:46.662+0000] {spark_submit.py:571} INFO - |  PL|https://crests.fo...|2021|2022-03-20T08:58:54Z|      Premier League|LEAGUE|     ENG|https://crests.fo...|  2072|      England|         2021|             PL|           1564|             22|    1564|          NULL|
[2024-01-29T14:11:46.662+0000] {spark_submit.py:571} INFO - | CLI|https://crests.fo...|2152|2023-03-16T16:22:43Z|   Copa Libertadores|   CUP|     SAM|https://crests.fo...|  2220|South America|         2152|            CLI|           1644|              1|    1644|          NULL|
[2024-01-29T14:11:46.662+0000] {spark_submit.py:571} INFO - +----+--------------------+----+--------------------+--------------------+------+--------+--------------------+------+-------------+-------------+---------------+---------------+---------------+--------+--------------+
[2024-01-29T14:11:46.662+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:46.973+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO FileSourceStrategy: Pushed Filters: IsNotNull(seasons)
[2024-01-29T14:11:46.975+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:46 INFO FileSourceStrategy: Post-Scan Filters: (size(seasons#7, true) > 0),isnotnull(seasons#7)
[2024-01-29T14:11:47.126+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.8 KiB, free 433.7 MiB)
[2024-01-29T14:11:47.183+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 25.1 KiB, free: 434.3 MiB)
[2024-01-29T14:11:47.188+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.7 MiB)
[2024-01-29T14:11:47.189+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 34.8 KiB, free: 434.3 MiB)
[2024-01-29T14:11:47.191+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO SparkContext: Created broadcast 7 from parquet at RawToFormatted.scala:73
[2024-01-29T14:11:47.198+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 27342128 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-29T14:11:47.217+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO DAGScheduler: Registering RDD 20 (parquet at RawToFormatted.scala:73) as input to shuffle 1
[2024-01-29T14:11:47.218+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO DAGScheduler: Got map stage job 4 (parquet at RawToFormatted.scala:73) with 2 output partitions
[2024-01-29T14:11:47.218+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at RawToFormatted.scala:73)
[2024-01-29T14:11:47.218+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO DAGScheduler: Parents of final stage: List()
[2024-01-29T14:11:47.218+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:11:47.222+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[20] at parquet at RawToFormatted.scala:73), which has no missing parents
[2024-01-29T14:11:47.227+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.1 KiB, free 433.6 MiB)
[2024-01-29T14:11:47.231+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 433.6 MiB)
[2024-01-29T14:11:47.232+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 24.4 KiB, free: 434.3 MiB)
[2024-01-29T14:11:47.233+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:11:47.234+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[20] at parquet at RawToFormatted.scala:73) (first 15 tasks are for partitions Vector(0, 1))
[2024-01-29T14:11:47.235+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2024-01-29T14:11:47.238+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, PROCESS_LOCAL, 9227 bytes)
[2024-01-29T14:11:47.239+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 1, PROCESS_LOCAL, 9102 bytes)
[2024-01-29T14:11:47.239+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
[2024-01-29T14:11:47.243+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2024-01-29T14:11:47.267+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 34.8 KiB, free: 434.3 MiB)
[2024-01-29T14:11:47.290+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/DED.json, range: 0-11649, partition values: [empty row]
[2024-01-29T14:11:47.291+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PL.json, range: 0-31329, partition values: [empty row]
[2024-01-29T14:11:47.544+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/EC.json, range: 0-9263, partition values: [empty row]
[2024-01-29T14:11:47.559+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BL1.json, range: 0-20528, partition values: [empty row]
[2024-01-29T14:11:47.781+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PD.json, range: 0-15902, partition values: [empty row]
[2024-01-29T14:11:47.942+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:47 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CL.json, range: 0-8210, partition values: [empty row]
[2024-01-29T14:11:48.007+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/SA.json, range: 0-15666, partition values: [empty row]
[2024-01-29T14:11:48.222+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/WC.json, range: 0-13533, partition values: [empty row]
[2024-01-29T14:11:48.327+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BSA.json, range: 0-3389, partition values: [empty row]
[2024-01-29T14:11:48.430+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PPL.json, range: 0-12814, partition values: [empty row]
[2024-01-29T14:11:48.543+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/ELC.json, range: 0-2334, partition values: [empty row]
[2024-01-29T14:11:48.643+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/FL1.json, range: 0-12632, partition values: [empty row]
[2024-01-29T14:11:48.935+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CLI.json, range: 0-1055, partition values: [empty row]
[2024-01-29T14:11:48.940+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2693 bytes result sent to driver
[2024-01-29T14:11:48.943+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:48 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1704 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (1/2)
[2024-01-29T14:11:49.398+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2693 bytes result sent to driver
[2024-01-29T14:11:49.401+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 2162 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (2/2)
[2024-01-29T14:11:49.401+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-01-29T14:11:49.404+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO DAGScheduler: ShuffleMapStage 5 (parquet at RawToFormatted.scala:73) finished in 2.182 s
[2024-01-29T14:11:49.404+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO DAGScheduler: looking for newly runnable stages
[2024-01-29T14:11:49.404+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO DAGScheduler: running: Set()
[2024-01-29T14:11:49.404+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO DAGScheduler: waiting: Set()
[2024-01-29T14:11:49.405+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO DAGScheduler: failed: Set()
[2024-01-29T14:11:49.413+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:49 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-01-29T14:11:50.301+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=653; previousMaxLatencyMs=0; operationCount=1; context=gs://data-lake-buck/formatted/football_data/competition/2024-01-29
[2024-01-29T14:11:50.348+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:50.392+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:50.393+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:50.394+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:50.395+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:50.395+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:50.397+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:50.766+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO GhfsStorageStatistics: Detected potential high latency for operation op_mkdirs. latencyMs=368; previousMaxLatencyMs=0; operationCount=1; context=gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0
[2024-01-29T14:11:50.791+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO SparkContext: Starting job: parquet at RawToFormatted.scala:73
[2024-01-29T14:11:50.795+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO DAGScheduler: Got job 5 (parquet at RawToFormatted.scala:73) with 1 output partitions
[2024-01-29T14:11:50.795+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at RawToFormatted.scala:73)
[2024-01-29T14:11:50.795+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2024-01-29T14:11:50.795+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:11:50.796+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[24] at parquet at RawToFormatted.scala:73), which has no missing parents
[2024-01-29T14:11:50.845+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 265.7 KiB, free 433.6 MiB)
[2024-01-29T14:11:50.849+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 96.8 KiB, free 433.5 MiB)
[2024-01-29T14:11:50.850+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 96.8 KiB, free: 434.2 MiB)
[2024-01-29T14:11:50.851+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:11:50.852+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[24] at parquet at RawToFormatted.scala:73) (first 15 tasks are for partitions Vector(0))
[2024-01-29T14:11:50.852+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-01-29T14:11:50.854+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, NODE_LOCAL, 7847 bytes)
[2024-01-29T14:11:50.855+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2024-01-29T14:11:50.918+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO ShuffleBlockFetcherIterator: Getting 2 (3.9 KiB) non-empty blocks including 2 (3.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-01-29T14:11:50.918+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2024-01-29T14:11:51.015+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO CodeGenerator: Code generated in 43.010815 ms
[2024-01-29T14:11:51.020+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:51.021+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:51.026+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:51.026+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:51.026+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:51.027+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:51.034+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO CodecConfig: Compression: SNAPPY
[2024-01-29T14:11:51.038+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO CodecConfig: Compression: SNAPPY
[2024-01-29T14:11:51.076+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-01-29T14:11:51.138+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-01-29T14:11:51.139+0000] {spark_submit.py:571} INFO - {
[2024-01-29T14:11:51.139+0000] {spark_submit.py:571} INFO - "type" : "struct",
[2024-01-29T14:11:51.139+0000] {spark_submit.py:571} INFO - "fields" : [ {
[2024-01-29T14:11:51.139+0000] {spark_submit.py:571} INFO - "name" : "code",
[2024-01-29T14:11:51.139+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.139+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.139+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - "name" : "emblem",
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - "name" : "id",
[2024-01-29T14:11:51.140+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - "name" : "lastUpdated",
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.141+0000] {spark_submit.py:571} INFO - "name" : "name",
[2024-01-29T14:11:51.142+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.142+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.142+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.142+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.143+0000] {spark_submit.py:571} INFO - "name" : "type",
[2024-01-29T14:11:51.143+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.143+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.143+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.143+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.143+0000] {spark_submit.py:571} INFO - "name" : "areaCode",
[2024-01-29T14:11:51.143+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.144+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.144+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.144+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.144+0000] {spark_submit.py:571} INFO - "name" : "areaFlag",
[2024-01-29T14:11:51.144+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.144+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.144+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.145+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.145+0000] {spark_submit.py:571} INFO - "name" : "areaId",
[2024-01-29T14:11:51.145+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:51.145+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.146+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.146+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.146+0000] {spark_submit.py:571} INFO - "name" : "areaName",
[2024-01-29T14:11:51.147+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.147+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.147+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.148+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.148+0000] {spark_submit.py:571} INFO - "name" : "competitionId",
[2024-01-29T14:11:51.148+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:51.148+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.148+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.148+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.148+0000] {spark_submit.py:571} INFO - "name" : "competitionCode",
[2024-01-29T14:11:51.149+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:51.149+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.149+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.149+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.152+0000] {spark_submit.py:571} INFO - "name" : "currentSeasonId",
[2024-01-29T14:11:51.152+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:51.152+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.152+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.152+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.152+0000] {spark_submit.py:571} INFO - "name" : "currentMatchDay",
[2024-01-29T14:11:51.153+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:51.154+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.154+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.154+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.155+0000] {spark_submit.py:571} INFO - "name" : "seasonId",
[2024-01-29T14:11:51.155+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:51.156+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.156+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.156+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:51.157+0000] {spark_submit.py:571} INFO - "name" : "seasonWinnerId",
[2024-01-29T14:11:51.157+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:51.157+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:51.157+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:51.157+0000] {spark_submit.py:571} INFO - } ]
[2024-01-29T14:11:51.157+0000] {spark_submit.py:571} INFO - }
[2024-01-29T14:11:51.158+0000] {spark_submit.py:571} INFO - and corresponding Parquet message type:
[2024-01-29T14:11:51.158+0000] {spark_submit.py:571} INFO - message spark_schema {
[2024-01-29T14:11:51.158+0000] {spark_submit.py:571} INFO - optional binary code (STRING);
[2024-01-29T14:11:51.159+0000] {spark_submit.py:571} INFO - optional binary emblem (STRING);
[2024-01-29T14:11:51.159+0000] {spark_submit.py:571} INFO - optional int64 id;
[2024-01-29T14:11:51.159+0000] {spark_submit.py:571} INFO - optional binary lastUpdated (STRING);
[2024-01-29T14:11:51.160+0000] {spark_submit.py:571} INFO - optional binary name (STRING);
[2024-01-29T14:11:51.160+0000] {spark_submit.py:571} INFO - optional binary type (STRING);
[2024-01-29T14:11:51.160+0000] {spark_submit.py:571} INFO - optional binary areaCode (STRING);
[2024-01-29T14:11:51.160+0000] {spark_submit.py:571} INFO - optional binary areaFlag (STRING);
[2024-01-29T14:11:51.160+0000] {spark_submit.py:571} INFO - optional int64 areaId;
[2024-01-29T14:11:51.163+0000] {spark_submit.py:571} INFO - optional binary areaName (STRING);
[2024-01-29T14:11:51.163+0000] {spark_submit.py:571} INFO - optional int64 competitionId;
[2024-01-29T14:11:51.163+0000] {spark_submit.py:571} INFO - optional binary competitionCode (STRING);
[2024-01-29T14:11:51.165+0000] {spark_submit.py:571} INFO - optional int64 currentSeasonId;
[2024-01-29T14:11:51.165+0000] {spark_submit.py:571} INFO - optional int64 currentMatchDay;
[2024-01-29T14:11:51.165+0000] {spark_submit.py:571} INFO - optional int64 seasonId;
[2024-01-29T14:11:51.165+0000] {spark_submit.py:571} INFO - optional int64 seasonWinnerId;
[2024-01-29T14:11:51.166+0000] {spark_submit.py:571} INFO - }
[2024-01-29T14:11:51.166+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:51.168+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:51.454+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=281; previousMaxLatencyMs=0; operationCount=1; context=gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0/_temporary/attempt_202401291411505669479265397914436_0007_m_000000_7/part-00000-dd326068-8305-43a3-bf19-9113bc4f9d70-c000.snappy.parquet
[2024-01-29T14:11:51.489+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:51 INFO CodecPool: Got brand-new compressor [.snappy]
[2024-01-29T14:11:52.484+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:52 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=210; previousMaxLatencyMs=0; operationCount=1; context=gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0/_temporary/attempt_202401291411505669479265397914436_0007_m_000000_7/part-00000-dd326068-8305-43a3-bf19-9113bc4f9d70-c000.snappy.parquet
[2024-01-29T14:11:54.212+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0/_temporary/' directory.
[2024-01-29T14:11:54.213+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO GhfsStorageStatistics: Detected potential high latency for operation op_rename. latencyMs=1133; previousMaxLatencyMs=0; operationCount=1; context=rename(gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0/_temporary/attempt_202401291411505669479265397914436_0007_m_000000_7 -> gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0/task_202401291411505669479265397914436_0007_m_000000)
[2024-01-29T14:11:54.213+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO FileOutputCommitter: Saved output of task 'attempt_202401291411505669479265397914436_0007_m_000000_7' to gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0/task_202401291411505669479265397914436_0007_m_000000
[2024-01-29T14:11:54.215+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO SparkHadoopMapRedUtil: attempt_202401291411505669479265397914436_0007_m_000000_7: Committed. Elapsed time: 1445 ms.
[2024-01-29T14:11:54.234+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 5944 bytes result sent to driver
[2024-01-29T14:11:54.237+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 3383 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (1/1)
[2024-01-29T14:11:54.237+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-01-29T14:11:54.240+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO DAGScheduler: ResultStage 7 (parquet at RawToFormatted.scala:73) finished in 3.440 s
[2024-01-29T14:11:54.240+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-29T14:11:54.240+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-01-29T14:11:54.240+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO DAGScheduler: Job 5 finished: parquet at RawToFormatted.scala:73, took 3.448277 s
[2024-01-29T14:11:54.242+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:54 INFO FileFormatWriter: Start to commit write Job 33b0b445-e944-45d5-ae94-715b326428c0.
[2024-01-29T14:11:55.648+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_temporary/0/task_202401291411505669479265397914436_0007_m_000000/' directory.
[2024-01-29T14:11:56.289+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://data-lake-buck/formatted/football_data/competition/2024-01-29/' directory.
[2024-01-29T14:11:56.667+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:56 INFO BlockManagerInfo: Removed broadcast_9_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 96.8 KiB, free: 434.3 MiB)
[2024-01-29T14:11:56.675+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:56 INFO BlockManagerInfo: Removed broadcast_8_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 24.4 KiB, free: 434.3 MiB)
[2024-01-29T14:11:56.858+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:56 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=360; previousMaxLatencyMs=210; operationCount=2; context=gs://data-lake-buck/formatted/football_data/competition/2024-01-29/_SUCCESS
[2024-01-29T14:11:56.971+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:56 INFO FileFormatWriter: Write Job 33b0b445-e944-45d5-ae94-715b326428c0 committed. Elapsed time: 2727 ms.
[2024-01-29T14:11:56.977+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:56 INFO FileFormatWriter: Finished processing stats for write job 33b0b445-e944-45d5-ae94-715b326428c0.
[2024-01-29T14:11:57.046+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:57 INFO FileSourceStrategy: Pushed Filters: IsNotNull(seasons)
[2024-01-29T14:11:57.047+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:57 INFO FileSourceStrategy: Post-Scan Filters: (size(seasons#7, true) > 0),isnotnull(seasons#7)
[2024-01-29T14:11:57.998+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:57.999+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:58.000+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:58.001+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:58.001+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:58.002+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:58.002+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:58.428+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO CodeGenerator: Code generated in 40.75272 ms
[2024-01-29T14:11:58.435+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 199.8 KiB, free 433.7 MiB)
[2024-01-29T14:11:58.454+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.8 KiB, free 433.7 MiB)
[2024-01-29T14:11:58.456+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 34.8 KiB, free: 434.3 MiB)
[2024-01-29T14:11:58.459+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SparkContext: Created broadcast 10 from parquet at RawToFormatted.scala:74
[2024-01-29T14:11:58.462+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 27342128 bytes, open cost is considered as scanning 4194304 bytes.
[2024-01-29T14:11:58.482+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SparkContext: Starting job: parquet at RawToFormatted.scala:74
[2024-01-29T14:11:58.483+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO DAGScheduler: Got job 6 (parquet at RawToFormatted.scala:74) with 2 output partitions
[2024-01-29T14:11:58.483+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at RawToFormatted.scala:74)
[2024-01-29T14:11:58.484+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO DAGScheduler: Parents of final stage: List()
[2024-01-29T14:11:58.484+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:11:58.485+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[28] at parquet at RawToFormatted.scala:74), which has no missing parents
[2024-01-29T14:11:58.505+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 229.4 KiB, free 433.5 MiB)
[2024-01-29T14:11:58.508+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 81.9 KiB, free 433.4 MiB)
[2024-01-29T14:11:58.509+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 81.9 KiB, free: 434.2 MiB)
[2024-01-29T14:11:58.510+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:11:58.510+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[28] at parquet at RawToFormatted.scala:74) (first 15 tasks are for partitions Vector(0, 1))
[2024-01-29T14:11:58.511+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0
[2024-01-29T14:11:58.513+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, PROCESS_LOCAL, 9238 bytes)
[2024-01-29T14:11:58.514+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 9) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 1, PROCESS_LOCAL, 9113 bytes)
[2024-01-29T14:11:58.525+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2024-01-29T14:11:58.525+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO Executor: Running task 1.0 in stage 8.0 (TID 9)
[2024-01-29T14:11:58.587+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO CodeGenerator: Code generated in 38.8278 ms
[2024-01-29T14:11:58.591+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:58.591+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:58.591+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:58.591+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:58.592+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:58.592+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:58.592+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO CodecConfig: Compression: SNAPPY
[2024-01-29T14:11:58.593+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO CodecConfig: Compression: SNAPPY
[2024-01-29T14:11:58.596+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-01-29T14:11:58.600+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:58.600+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:58.605+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-01-29T14:11:58.605+0000] {spark_submit.py:571} INFO - {
[2024-01-29T14:11:58.605+0000] {spark_submit.py:571} INFO - "type" : "struct",
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - "fields" : [ {
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - "name" : "competitionId",
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - "name" : "competitionCode",
[2024-01-29T14:11:58.606+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:58.607+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.607+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.607+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.607+0000] {spark_submit.py:571} INFO - "name" : "id",
[2024-01-29T14:11:58.607+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.607+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - "name" : "endDate",
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - "name" : "startDate",
[2024-01-29T14:11:58.608+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - "name" : "winnerTeamId",
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.609+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - "name" : "currentMatchDay",
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - } ]
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - }
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - and corresponding Parquet message type:
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - message spark_schema {
[2024-01-29T14:11:58.610+0000] {spark_submit.py:571} INFO - optional int64 competitionId;
[2024-01-29T14:11:58.611+0000] {spark_submit.py:571} INFO - optional binary competitionCode (STRING);
[2024-01-29T14:11:58.611+0000] {spark_submit.py:571} INFO - optional int64 id;
[2024-01-29T14:11:58.611+0000] {spark_submit.py:571} INFO - optional binary endDate (STRING);
[2024-01-29T14:11:58.611+0000] {spark_submit.py:571} INFO - optional binary startDate (STRING);
[2024-01-29T14:11:58.611+0000] {spark_submit.py:571} INFO - optional int64 winnerTeamId;
[2024-01-29T14:11:58.611+0000] {spark_submit.py:571} INFO - optional int64 currentMatchDay;
[2024-01-29T14:11:58.613+0000] {spark_submit.py:571} INFO - }
[2024-01-29T14:11:58.614+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:58.614+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:58.618+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:58.618+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2024-01-29T14:11:58.619+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2024-01-29T14:11:58.619+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-01-29T14:11:58.619+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/DED.json, range: 0-11649, partition values: [empty row]
[2024-01-29T14:11:58.837+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO CodecConfig: Compression: SNAPPY
[2024-01-29T14:11:58.839+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO CodecConfig: Compression: SNAPPY
[2024-01-29T14:11:58.842+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2024-01-29T14:11:58.848+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2024-01-29T14:11:58.848+0000] {spark_submit.py:571} INFO - {
[2024-01-29T14:11:58.848+0000] {spark_submit.py:571} INFO - "type" : "struct",
[2024-01-29T14:11:58.848+0000] {spark_submit.py:571} INFO - "fields" : [ {
[2024-01-29T14:11:58.848+0000] {spark_submit.py:571} INFO - "name" : "competitionId",
[2024-01-29T14:11:58.848+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.849+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.849+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.849+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.849+0000] {spark_submit.py:571} INFO - "name" : "competitionCode",
[2024-01-29T14:11:58.849+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:58.849+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.849+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.850+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.850+0000] {spark_submit.py:571} INFO - "name" : "id",
[2024-01-29T14:11:58.850+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.851+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.851+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.851+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.851+0000] {spark_submit.py:571} INFO - "name" : "endDate",
[2024-01-29T14:11:58.852+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:58.852+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.852+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.852+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - "name" : "startDate",
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - "type" : "string",
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - "name" : "winnerTeamId",
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.853+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.854+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.854+0000] {spark_submit.py:571} INFO - }, {
[2024-01-29T14:11:58.854+0000] {spark_submit.py:571} INFO - "name" : "currentMatchDay",
[2024-01-29T14:11:58.854+0000] {spark_submit.py:571} INFO - "type" : "long",
[2024-01-29T14:11:58.854+0000] {spark_submit.py:571} INFO - "nullable" : true,
[2024-01-29T14:11:58.854+0000] {spark_submit.py:571} INFO - "metadata" : { }
[2024-01-29T14:11:58.855+0000] {spark_submit.py:571} INFO - } ]
[2024-01-29T14:11:58.863+0000] {spark_submit.py:571} INFO - }
[2024-01-29T14:11:58.864+0000] {spark_submit.py:571} INFO - and corresponding Parquet message type:
[2024-01-29T14:11:58.867+0000] {spark_submit.py:571} INFO - message spark_schema {
[2024-01-29T14:11:58.867+0000] {spark_submit.py:571} INFO - optional int64 competitionId;
[2024-01-29T14:11:58.867+0000] {spark_submit.py:571} INFO - optional binary competitionCode (STRING);
[2024-01-29T14:11:58.868+0000] {spark_submit.py:571} INFO - optional int64 id;
[2024-01-29T14:11:58.868+0000] {spark_submit.py:571} INFO - optional binary endDate (STRING);
[2024-01-29T14:11:58.868+0000] {spark_submit.py:571} INFO - optional binary startDate (STRING);
[2024-01-29T14:11:58.869+0000] {spark_submit.py:571} INFO - optional int64 winnerTeamId;
[2024-01-29T14:11:58.869+0000] {spark_submit.py:571} INFO - optional int64 currentMatchDay;
[2024-01-29T14:11:58.869+0000] {spark_submit.py:571} INFO - }
[2024-01-29T14:11:58.870+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:58.870+0000] {spark_submit.py:571} INFO - 
[2024-01-29T14:11:58.917+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=303; previousMaxLatencyMs=281; operationCount=4; context=gs://data-lake-buck/formatted/football_data/season/2024-01-29/_temporary/0/_temporary/attempt_202401291411582144548455331391872_0008_m_000000_8/part-00000-9987fc1e-a1e4-4411-a414-04c55423535c-c000.snappy.parquet
[2024-01-29T14:11:58.918+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:58 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PL.json, range: 0-31329, partition values: [empty row]
[2024-01-29T14:11:59.140+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:59 INFO CodecPool: Got brand-new compressor [.snappy]
[2024-01-29T14:11:59.148+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:59 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/EC.json, range: 0-9263, partition values: [empty row]
[2024-01-29T14:11:59.166+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:59 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BL1.json, range: 0-20528, partition values: [empty row]
[2024-01-29T14:11:59.248+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:59 INFO BlockManagerInfo: Removed broadcast_7_piece0 on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 in memory (size: 34.8 KiB, free: 434.3 MiB)
[2024-01-29T14:11:59.420+0000] {spark_submit.py:571} INFO - 24/01/29 14:11:59 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PD.json, range: 0-15902, partition values: [empty row]
[2024-01-29T14:22:19.793+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 INFO GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=446716; previousMaxLatencyMs=275; operationCount=45; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PD.json
[2024-01-29T14:22:19.804+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 INFO GhfsStorageStatistics: Detected potential high latency for operation stream_read_operations. latencyMs=620360; previousMaxLatencyMs=279; operationCount=91; context=gs://data-lake-buck/raw/football_data/competitions/2024-01-29/EC.json
[2024-01-29T14:22:19.804+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CL.json, range: 0-8210, partition values: [empty row]
[2024-01-29T14:22:19.836+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 311559 ms exceeds timeout 120000 ms
[2024-01-29T14:22:19.838+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395 in 120 seconds
[2024-01-29T14:22:19.991+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN NettyRpcEnv: Ignored message: true
[2024-01-29T14:22:19.992+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN NettyRpcEnv: Ignored message: true
[2024-01-29T14:22:19.993+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN NettyRpcEnv: Ignored message: true
[2024-01-29T14:22:19.993+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN NettyRpcEnv: Ignored message: true
[2024-01-29T14:22:19.994+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN NettyRpcEnv: Ignored message: true
[2024-01-29T14:22:19.994+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:19 WARN NettyRpcEnv: Ignored message: true
[2024-01-29T14:22:20.018+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:20.019+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:20.019+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:20.029+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN SparkContext: Killing executors is not supported by current scheduler.
[2024-01-29T14:22:20.052+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:20.052+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:20.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.054+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:20.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:20.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:20.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:20.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:20.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:20.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:20.057+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:20.057+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:20.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:20.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:20.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:20.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:20.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:20.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.089+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.167+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.183+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:20.183+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.183+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:20.183+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:20.183+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:20.184+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:20.184+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:20.184+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:20.184+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:20.184+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:20.184+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:20.184+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.247+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.247+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.247+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.248+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:20.248+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:20.250+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:20.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:20.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:20.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:20.251+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.298+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.298+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.300+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.301+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.301+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.301+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.301+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.301+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.301+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:20.302+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:20.302+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:20.367+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:20.367+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:20.368+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:20.368+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:20.368+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 ERROR Inbox: Ignoring error
[2024-01-29T14:22:20.368+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.368+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.368+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.368+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.403+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:20.403+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:20.403+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:20.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:20.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:20.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:20.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:20.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:20.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:20.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:20.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:20.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:20.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:20.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:20.405+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:20.458+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:20.458+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:20.458+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:20.458+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:20.458+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:20.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.460+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:20.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:20.543+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:20.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:20.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:20.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:20.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:20.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:20.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:20.561+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:20.561+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:20.561+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:20.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.561+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:20.562+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:20.563+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:20.600+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:20.600+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:20.600+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 ERROR Inbox: Ignoring error
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.601+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.602+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.602+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:20.634+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:20.635+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:20.635+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:20.635+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:20.652+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:20.652+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:20.652+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:20.652+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:20.652+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:20.653+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:20.653+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:20.653+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:20.653+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:20.653+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:20.653+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:20.653+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:20.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:20.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:20.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:20.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:20.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:20.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:20.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:20.684+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:20.709+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:20.709+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:20.710+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.710+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.710+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.710+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.710+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:20.710+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:20.710+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:20.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:20.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:20.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:20.711+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:20.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:20.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:20.711+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:20.712+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:20.712+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:20.712+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:20.712+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:20.712+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:20.744+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.744+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.744+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.745+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.745+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:20.745+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:20.745+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:20.746+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:20.746+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:20.746+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:20.746+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:20.746+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:20.747+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:20.747+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:20.783+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:20.783+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:20.784+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:20.784+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:20.784+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:20.784+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:20.784+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:20.784+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:20.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:20.786+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:20.786+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.786+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.819+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.819+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.819+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:20.819+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:20.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:20.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:20.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:20.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:20.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:20.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:20.821+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:20.821+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:20.821+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:20.821+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:20.842+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:20.847+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 ERROR Inbox: Ignoring error
[2024-01-29T14:22:20.847+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.847+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.847+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.847+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.848+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:20.848+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:20.851+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:20.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:20.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:20.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:20.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:20.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:20.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:20.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:20.861+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:20.861+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:20.862+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:20.862+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:20.862+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:20.862+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:20.862+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:20.866+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:20.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:20.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:20.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:20.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:20.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.869+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.873+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.873+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.873+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:20.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.882+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.882+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.882+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.883+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.901+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.901+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:20.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:20.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:20.904+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:20.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:20.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:20.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:20.911+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:20.911+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:20.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:20.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:20.919+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:20.919+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:20.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:20.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:20.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:20.920+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.920+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:20.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:20.925+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:20.925+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:20.927+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:20.929+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:20.929+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:20.929+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:20.929+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:20.930+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:20.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:20.941+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:20.941+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:20.941+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:20.941+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:20.941+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:20.942+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.947+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.948+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:20.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:20.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:20.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:20.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:20.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:20.953+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:20.961+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:20.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:20.962+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:20.962+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:20.962+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:20.962+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:20.962+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:20.966+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:20.966+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:20.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:20.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:20.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:20.975+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:20.976+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:20.976+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:20.976+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:20.976+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:20.979+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:20.981+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:20.981+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:20.986+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:20.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:20.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:20.988+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:20.988+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:20.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:20.991+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:20.991+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:20.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:20.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:20.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:20.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:20.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:20.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:20.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.011+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.016+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.016+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.016+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.016+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.016+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.016+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.016+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.026+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.031+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.031+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.036+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.036+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.049+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.051+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.051+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:21.051+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:21.051+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:21.054+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 ERROR Inbox: Ignoring error
[2024-01-29T14:22:21.054+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.054+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.054+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.091+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.092+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.093+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:21.093+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:21.093+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:21.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:21.141+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:21.141+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:21.141+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:21.141+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:21.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:21.149+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:21.150+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:21.150+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.151+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.153+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.153+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.156+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:21.158+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:21.160+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:21.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:21.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:21.166+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:21.175+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.181+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:21.181+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:21.181+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.182+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.183+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.183+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.183+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.189+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.189+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.189+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.189+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.198+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.198+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.198+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.198+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.198+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.198+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.198+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.199+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.199+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:21.204+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.204+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.204+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.204+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.204+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.204+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.204+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:21.205+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:21.211+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:21.211+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:21.219+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:21.223+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:21.223+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:21.225+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:21.225+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:21.225+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 ERROR Inbox: Ignoring error
[2024-01-29T14:22:21.226+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.226+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.226+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.228+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.228+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.228+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.229+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.229+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.233+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.234+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.234+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.234+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.234+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.234+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.234+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.236+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.237+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.237+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.237+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.237+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.237+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.237+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.237+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.243+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.243+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.243+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.244+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.249+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.249+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.249+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.249+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.249+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.249+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.249+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.255+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.255+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.255+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.256+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.256+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.256+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.267+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.267+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.267+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.267+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.267+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.267+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.268+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.268+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.268+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.268+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.268+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.268+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.268+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.269+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.269+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.269+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.269+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.284+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.284+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.284+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.284+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.285+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.299+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.300+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.300+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.300+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.300+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.300+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.300+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:21.304+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.304+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.304+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.304+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.304+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:21.305+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:21.305+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:21.311+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:21.316+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:21.316+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:21.335+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.335+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:21.335+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:21.335+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:21.335+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:21.335+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:21.336+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.336+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.336+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.336+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.336+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.336+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.339+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.339+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.339+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.339+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.339+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.340+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.340+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.355+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.355+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.356+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.356+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.356+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.356+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.360+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.360+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.360+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:21.365+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.365+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.366+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.366+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.367+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.367+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.367+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.367+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.381+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.381+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.384+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.384+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.384+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.384+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.384+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.384+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.384+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.386+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.386+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.388+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.388+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.389+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.390+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.390+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.390+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.391+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.391+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.391+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.391+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.392+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.392+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.393+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.393+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.393+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.394+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.394+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.394+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.394+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.394+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.395+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.396+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.396+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.396+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.396+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.400+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.400+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.401+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.402+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.402+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:21.402+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:21.402+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:21.402+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 ERROR Inbox: Ignoring error
[2024-01-29T14:22:21.402+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.404+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.411+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.412+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.413+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.413+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.413+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.413+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.413+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.413+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.413+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.419+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.419+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.420+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.422+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.422+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.423+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.423+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.424+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.426+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.426+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.426+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.426+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.426+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.427+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.428+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.428+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.429+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.429+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.429+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.429+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.430+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.435+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.435+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.435+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.444+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.445+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.455+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.455+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.455+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.456+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.456+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.461+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.461+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.461+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.461+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.461+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.466+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.466+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.466+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.471+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.471+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.471+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.471+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.478+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.478+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.478+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.478+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.483+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/SA.json, range: 0-15666, partition values: [empty row]
[2024-01-29T14:22:21.485+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:21.485+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.485+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.485+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.486+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.486+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:21.486+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:21.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:21.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:21.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:21.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:21.491+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.495+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:21.495+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:21.498+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:21.501+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:21.502+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:21.502+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.502+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.502+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.502+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.502+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.506+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.506+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.509+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.509+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.509+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.509+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.509+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.509+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.510+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.516+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.516+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.516+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.516+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.516+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.516+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:21.524+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.524+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.524+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.527+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.528+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.528+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.528+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.537+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.538+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.538+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.538+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.538+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.539+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.560+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.566+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.566+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.566+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.566+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.572+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.573+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.574+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.574+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.574+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.574+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.574+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.575+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.575+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.579+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.586+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.592+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.592+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/BSA.json, range: 0-3389, partition values: [empty row]
[2024-01-29T14:22:21.592+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:21.592+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:21.596+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:21.597+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 ERROR Inbox: Ignoring error
[2024-01-29T14:22:21.598+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.599+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.600+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.600+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.630+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.633+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.644+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.644+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.644+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.644+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.644+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.645+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.646+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.646+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.660+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.661+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.661+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.661+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.664+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.664+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.664+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.674+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.675+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.676+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.676+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.676+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.676+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.680+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.686+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.686+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.687+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.687+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.689+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.691+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.692+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.719+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.719+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.723+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.723+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.728+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.728+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.728+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.729+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.729+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.732+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.735+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.737+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.737+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.737+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/WC.json, range: 0-13533, partition values: [empty row]
[2024-01-29T14:22:21.737+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:20 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:21.737+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.737+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:21.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:21.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:21.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:21.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:21.752+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:21.752+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.752+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:21.752+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:21.755+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:21.755+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:21.758+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:21.758+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.758+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.759+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.759+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.760+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.760+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.760+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.761+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.761+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.761+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.763+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.770+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.771+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.771+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.771+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.771+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:21.772+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.772+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.772+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.776+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.782+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.782+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.782+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.782+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.782+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.782+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.783+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.785+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.786+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.786+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.790+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.790+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.792+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.793+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.793+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.793+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.793+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.793+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.793+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.796+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.796+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.796+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.797+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.797+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.797+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.800+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.802+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.802+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.804+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.805+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.805+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.805+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.805+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.805+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.805+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.809+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.811+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.813+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.813+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.813+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.814+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.814+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.814+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.814+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.819+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/ELC.json, range: 0-2334, partition values: [empty row]
[2024-01-29T14:22:21.819+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:21.819+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:21.819+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:21.819+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 ERROR Inbox: Ignoring error
[2024-01-29T14:22:21.819+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.823+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.824+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.824+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.824+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.849+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.855+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.855+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.859+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.859+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.859+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.859+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.860+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.860+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.868+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.868+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.868+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.869+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.869+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.869+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.874+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.874+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.881+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.881+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.881+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.881+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.881+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.884+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.889+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.889+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.889+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.889+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.889+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.889+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.892+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.898+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.898+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.899+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.899+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.899+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.899+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.901+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.901+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.903+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.904+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.906+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.912+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.912+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.912+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.912+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/PPL.json, range: 0-12814, partition values: [empty row]
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.913+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:21.914+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.915+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.917+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.919+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.919+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.936+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.937+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.938+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.948+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.950+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.951+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.956+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:21.956+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:21.958+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:21.958+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 ERROR Inbox: Ignoring error
[2024-01-29T14:22:21.958+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.958+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:21.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:21.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:21.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:21.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:21.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:21.963+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:21.963+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:21.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:21.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:21.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:21.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:21.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:21.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.974+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.983+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.983+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.983+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:21.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:21.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:21.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:21.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:21.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:21.992+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:21.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:21.992+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:21.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:21.994+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:21.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:21.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.002+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.003+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.003+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.003+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.003+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.003+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.003+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.003+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.004+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.007+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.007+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.021+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.021+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.021+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.023+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.023+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.027+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.033+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.033+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.033+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.034+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.042+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.043+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.043+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.043+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.043+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.046+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.046+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.046+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.047+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.050+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.051+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.051+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.051+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.051+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.051+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.058+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.060+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.061+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.072+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.072+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.072+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.124+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.125+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.125+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.137+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.137+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.137+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.137+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.137+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/CLI.json, range: 0-1055, partition values: [empty row]
[2024-01-29T14:22:22.137+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.152+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.152+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.152+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.152+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.152+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.152+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.153+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.154+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.154+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.154+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.154+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.155+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.155+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.179+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.181+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.182+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.183+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.183+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.206+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.206+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.206+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.206+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.206+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.207+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.207+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.207+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.207+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.207+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.208+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.209+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.209+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.209+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.209+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.209+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.257+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.257+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.262+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.262+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.266+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.266+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.266+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.266+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.266+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.266+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.266+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.267+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.270+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.271+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.271+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.277+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.277+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.277+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.282+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.282+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.282+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.282+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.283+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.286+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.287+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.303+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.303+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.303+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.303+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.304+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.305+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.307+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.307+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.329+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.329+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.331+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.331+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.331+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.332+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO FileScanRDD: Reading File path: gs://data-lake-buck/raw/football_data/competitions/2024-01-29/FL1.json, range: 0-12632, partition values: [empty row]
[2024-01-29T14:22:22.333+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.333+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.335+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.336+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.336+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.336+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.338+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.338+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.339+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.344+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.350+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.350+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.350+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.350+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.350+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.350+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.351+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.368+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.368+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.368+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.368+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.369+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.369+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.369+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.369+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.375+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.376+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.376+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.376+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.377+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.383+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.383+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.385+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.386+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.391+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.391+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.392+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.392+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.392+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.392+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.392+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.392+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.395+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.395+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.395+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.395+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.402+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.404+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.404+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.404+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.407+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.407+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.424+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.424+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.424+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.424+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.425+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.425+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.425+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.426+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.426+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.429+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.430+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.431+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.432+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.432+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.433+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.433+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.433+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.433+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.434+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.435+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.435+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.435+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.435+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.436+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.436+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.436+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.436+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.436+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.438+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.438+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.439+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.439+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.439+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.440+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.440+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.440+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.440+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.440+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.442+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.442+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.442+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.442+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.442+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.442+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.442+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.445+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.446+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.448+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.448+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.451+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.451+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.451+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.451+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.453+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.453+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.453+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.454+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.454+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.455+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.455+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.455+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.457+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.459+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.460+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.461+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.461+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.462+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.462+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.462+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.462+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.462+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.463+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.466+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.472+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.472+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.472+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.472+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.472+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.472+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.475+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.478+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.479+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.479+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.480+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.481+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.485+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.489+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.492+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.492+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.493+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.493+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.493+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.493+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.494+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.495+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.497+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.498+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.499+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.499+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:21 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.499+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.499+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.499+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.500+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.501+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.501+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.501+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.501+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.501+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.501+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.503+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.503+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.503+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.509+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.510+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.510+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.510+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.516+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.517+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.521+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.529+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.529+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.529+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.530+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.530+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.530+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.541+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.542+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.542+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.542+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.542+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.542+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.542+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.542+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.544+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.552+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.552+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.553+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.553+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.553+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.553+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.553+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.563+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.563+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.564+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.564+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.566+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.566+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.566+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.567+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.567+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.567+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.567+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.578+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.580+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.581+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.581+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.581+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.582+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.582+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.582+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.582+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.586+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.586+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.586+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.586+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.586+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.586+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.592+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.592+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.592+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.592+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.596+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.597+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.598+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.599+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.608+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.609+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.609+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.609+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.609+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.609+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.609+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.615+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.615+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.615+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.616+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.619+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:22.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:22.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:22.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:22.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:22.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:22.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:22.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:22.624+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:22.624+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:22.625+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.630+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.631+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.636+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.636+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.655+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.655+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.655+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.656+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.657+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.658+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.658+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.658+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.658+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.658+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.658+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.659+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.659+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.660+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.674+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.676+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.679+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.686+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.688+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.693+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.695+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:22.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:22.700+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:22.700+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:22.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:22.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:22.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:22.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:22.709+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:22.712+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:22.713+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.713+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.716+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.735+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.735+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.735+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.736+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.736+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.736+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.736+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.744+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.744+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.745+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.745+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.745+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.759+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.759+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.759+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.759+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.760+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.760+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.760+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.768+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.768+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.769+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.769+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.769+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.774+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.785+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.785+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.789+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.795+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.796+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.796+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.796+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.796+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.802+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.809+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.809+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.809+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.811+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.826+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.826+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.826+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.826+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.827+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.827+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.829+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.829+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.829+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.833+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.836+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.836+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.836+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.836+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.836+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.839+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.839+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.839+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.839+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.839+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.840+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.840+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.842+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.842+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.842+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.842+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.843+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.843+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.847+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.849+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.851+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.852+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.852+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.852+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.852+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.852+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.852+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.857+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.857+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.858+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.861+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.861+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.864+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.864+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.864+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.864+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.864+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.864+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.865+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.865+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.872+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.873+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.873+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.875+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.875+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.876+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.879+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.880+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.880+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.880+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.880+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.882+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.883+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.883+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.883+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.883+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.883+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.890+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.890+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.891+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.893+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.893+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.893+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.893+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.894+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.894+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.894+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.894+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.896+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.896+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.899+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.899+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.901+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.901+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.901+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.901+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.904+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.905+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.905+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.909+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.909+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.909+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.917+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.917+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.917+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.917+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.917+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.918+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.919+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.919+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.919+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.920+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.920+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.920+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.920+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.921+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.921+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.922+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.922+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.925+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.926+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.926+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.926+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.926+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.926+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.926+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.930+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.930+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.930+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.930+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.933+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.933+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.933+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.933+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.933+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.934+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.934+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.935+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.935+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.935+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.935+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.940+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.940+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.940+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.940+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.943+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.943+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.943+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.943+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.945+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.945+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.945+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.945+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.946+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.946+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.946+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.946+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.946+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.946+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.946+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.952+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.952+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.953+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.953+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.953+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.953+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.953+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.953+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.953+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.954+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.955+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.957+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.957+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.957+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.957+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.957+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.957+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.957+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.958+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.958+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.958+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.958+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.958+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.959+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:22.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:22.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:22.960+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:22.960+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:22.960+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:22.960+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:22.960+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:22.960+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:22.961+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:22.961+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.961+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.961+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.961+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.961+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.961+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.963+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.966+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:22.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:22.968+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.969+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.971+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.971+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.971+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.971+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.971+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.971+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.971+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:22.972+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:22.972+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:22.972+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.982+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.982+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.983+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.985+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:22.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:22.989+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:22.989+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:22.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:22.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:22.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:22.989+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:22.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:22.990+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:22.991+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:22.991+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:22.991+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:22.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:22.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:22.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:22.991+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:22.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:22.993+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:22.994+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:22.994+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:22.994+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:22.994+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:22.994+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:22.994+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:22.994+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:22.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:22.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:22.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:22.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:22.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:22.995+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:22.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:22.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:22.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:22.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:22.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:22.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:22.997+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:22.997+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:22.997+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:22.997+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:22.997+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:22.997+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:22.998+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.006+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.006+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.007+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.007+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.007+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.007+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.007+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.007+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.007+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.008+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.010+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.013+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.014+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.014+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.014+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.017+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.018+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.023+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.023+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.023+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.023+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.024+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.026+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.027+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.028+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.031+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.031+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.031+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.031+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.031+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.031+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.032+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.033+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.033+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.033+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.033+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.033+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.033+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.036+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.038+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.039+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.040+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.041+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.042+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.044+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.045+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.046+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.046+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.046+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.046+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.046+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.047+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.047+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.047+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.050+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.050+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.050+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.050+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.050+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.054+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.056+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.057+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.058+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.059+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.059+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.059+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.059+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.059+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.059+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.062+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.064+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.065+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.066+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.070+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.071+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.072+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.076+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.076+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.076+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.076+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.076+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.076+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.076+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.077+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.077+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.077+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.078+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.078+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.078+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.082+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.084+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.085+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.089+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:22 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.091+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.091+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.091+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.091+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.091+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.092+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.092+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.092+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.097+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.100+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.102+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.105+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.105+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.105+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.105+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.107+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.108+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.111+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.112+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.114+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.116+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.117+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.120+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.127+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.127+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.128+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.129+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.130+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.131+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.131+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.131+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.131+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.131+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.131+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.132+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.134+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:23.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:23.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:23.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.135+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.137+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.138+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.140+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.140+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.142+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.142+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:23.143+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:23.144+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:23.145+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.145+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.145+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.145+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.145+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.145+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.145+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.150+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.151+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.151+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.151+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.151+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.151+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.151+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.151+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.152+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.153+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.154+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.155+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.155+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.156+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.157+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.157+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.157+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.157+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.157+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.157+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.157+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.158+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.158+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.158+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.158+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.159+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.160+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.161+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.162+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.163+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.164+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.165+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.166+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.168+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.168+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.169+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.170+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.171+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.172+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.173+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.174+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.175+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.175+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.175+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.175+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.175+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.175+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.175+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.176+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.177+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.177+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.177+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.177+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.177+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.177+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.178+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.179+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.179+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.180+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.195+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.195+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.195+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.196+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.196+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.197+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.197+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.197+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.197+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.197+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.198+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.198+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.199+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.199+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.199+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.200+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.200+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.200+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.200+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.201+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.201+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.201+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.202+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.203+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.204+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.204+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.204+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.204+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.204+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.204+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.205+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.205+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.206+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.206+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.206+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.206+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.206+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.206+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.206+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.209+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.209+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.209+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.209+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.209+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.209+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.210+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.210+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.213+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.213+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.213+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.214+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.214+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.214+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.214+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.215+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.215+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.215+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.216+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.216+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.216+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.216+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.216+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.219+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.220+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.220+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.220+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.220+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.220+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.220+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.220+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.223+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.223+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.223+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.223+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.223+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.223+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.224+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.226+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.226+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.226+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.226+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.227+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.227+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.229+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.229+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.230+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.230+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.230+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.230+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.230+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.231+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.231+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.232+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.232+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.232+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.232+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.232+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.232+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.232+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.234+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.234+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.240+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.241+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.241+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.241+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.242+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.242+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.242+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.242+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.243+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.243+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.243+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.243+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.244+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.245+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.245+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.246+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.246+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.246+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.246+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.246+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.246+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.248+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.248+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.248+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.250+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.250+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.250+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.250+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.250+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.250+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.251+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.258+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.258+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.258+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.258+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.258+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.259+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.260+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.260+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.260+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.260+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.260+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.262+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.262+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.262+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.263+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.263+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.263+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.263+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.264+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.264+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.264+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:23.265+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:23.265+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:23.265+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:23.265+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:23.265+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:23.266+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:23.267+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:23.267+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:23.267+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:23.267+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.267+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.267+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.268+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.269+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.271+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.273+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.273+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.273+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.273+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.273+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.273+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.273+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.275+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.275+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.275+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.275+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.276+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.277+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.277+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.277+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.277+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.279+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.279+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-01-29T14:22:23.280+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - ... 17 more
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.281+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.282+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.283+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.283+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.287+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.287+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.287+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.287+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.287+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.288+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.289+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.290+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.291+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.291+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.295+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.295+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.295+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.295+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.296+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.297+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.297+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.297+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.297+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.305+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.306+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.306+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.306+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.307+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.307+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.308+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.308+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.308+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.308+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.308+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.309+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.309+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.309+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.309+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.309+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.309+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.310+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.310+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.310+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.310+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.310+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.310+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.310+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.314+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.314+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.314+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.314+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.315+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.315+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.316+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.316+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.316+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.317+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.318+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.319+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.320+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.320+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.320+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.320+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.321+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.322+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.323+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.324+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.324+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.324+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.324+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.324+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.330+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.330+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.330+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.330+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.330+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.331+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.339+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.340+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.341+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.341+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.341+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.341+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.341+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.341+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.341+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.342+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.343+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.344+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.345+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.347+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.348+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.348+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.348+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.351+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.351+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.351+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.352+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.352+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.352+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.352+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.352+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.353+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.353+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.353+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.353+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.353+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.353+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.354+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.355+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.364+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.365+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.366+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.368+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.368+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.368+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.368+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.368+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.368+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.368+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.372+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.372+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.372+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.372+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.373+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.373+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.373+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.377+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.377+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.377+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.377+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.378+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.379+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.382+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.383+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.385+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.385+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.385+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.385+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.386+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.386+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.387+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.387+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.388+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.388+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.388+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.389+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.390+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.391+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.391+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.391+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.391+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.391+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.391+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.391+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.392+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.393+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.394+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.394+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.394+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.395+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.396+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.396+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.396+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.396+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.396+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.396+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.397+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.397+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.398+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.398+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.399+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.399+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.399+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.399+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.400+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.402+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.402+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.403+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.403+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.403+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.403+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.404+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.405+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.405+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.406+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.407+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.408+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.408+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.409+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.409+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.409+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.409+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.409+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.409+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.410+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.411+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.411+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.411+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.411+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.411+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.412+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.413+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.421+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.421+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.421+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.421+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.423+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.423+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.425+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.425+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.425+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.425+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.425+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.426+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.426+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.427+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.427+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.428+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.428+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.428+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.428+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.429+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.430+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.430+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.431+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.431+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.431+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.431+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.432+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.433+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.433+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.433+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.433+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.433+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.433+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.434+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.434+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.435+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.435+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.435+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.435+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.435+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.435+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.437+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.437+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.437+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.437+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.438+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.439+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.439+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.439+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.439+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.440+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.440+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.440+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.444+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.445+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.445+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.447+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.450+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.452+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.453+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.454+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.454+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.454+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.454+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.454+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.456+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.456+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.456+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.456+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.457+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.457+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.457+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.458+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.458+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.458+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.459+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.459+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.459+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.459+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.461+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.461+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.461+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.462+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.463+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.463+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.463+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.463+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.463+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.463+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.465+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.465+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.465+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.465+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.465+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.465+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.466+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.467+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.467+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.467+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.467+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.468+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.468+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.468+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.468+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.469+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.470+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.470+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.470+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.470+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.470+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.470+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.471+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.471+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.471+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.471+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.471+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.472+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.473+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.474+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.475+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.476+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.477+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.478+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.479+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.480+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.481+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.482+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.482+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.482+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.482+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.482+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.483+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.484+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.485+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.486+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.486+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.487+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.488+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.489+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.490+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.491+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.492+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.493+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.494+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.495+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.496+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.497+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.498+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.499+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.500+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.501+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.502+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.502+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.502+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.502+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.502+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.502+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.502+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.503+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.504+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.505+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.505+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.505+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.505+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.505+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.505+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.506+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.507+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.508+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.509+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.509+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.509+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.509+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.509+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.509+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.509+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.510+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.511+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.512+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.513+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.514+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.515+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.515+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.516+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.517+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.518+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.519+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.520+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.521+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.522+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.523+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.524+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.525+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.526+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.526+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.526+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.526+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.526+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.526+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.526+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.527+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.527+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.527+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.527+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.527+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.527+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.527+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.528+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.529+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.529+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.529+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.529+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.529+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.529+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.530+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.531+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.532+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.533+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.534+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.535+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.535+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.535+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.535+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.535+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.535+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.535+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.536+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.536+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.536+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.536+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.536+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.536+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.536+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.537+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.537+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.537+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.537+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.537+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.537+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.537+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.538+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.538+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.538+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.538+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.538+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.539+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.540+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.541+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.542+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.543+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.544+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.545+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.546+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.547+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.548+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.549+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.549+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.549+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.549+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.549+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.549+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.549+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.550+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.551+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.552+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.553+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.553+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.553+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.553+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.553+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.553+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.553+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.554+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.555+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.556+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.557+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.558+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.559+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.560+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.561+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.562+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.563+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.564+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.565+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.566+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.567+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.568+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.569+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.569+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.569+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.569+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.569+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.569+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.569+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.570+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.571+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.572+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.573+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.574+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.575+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.576+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.576+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.580+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.581+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.582+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.583+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.584+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.585+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.586+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.587+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.588+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.589+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.590+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.590+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.590+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.590+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.590+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.590+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.590+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.591+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.592+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.593+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.594+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.595+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.596+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.597+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.598+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.599+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.600+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.601+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.602+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.603+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.604+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.605+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.606+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.607+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.608+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.609+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.610+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.610+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.610+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.610+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.610+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.610+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.610+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.611+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.612+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.613+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.614+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.615+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.615+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.615+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.615+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.615+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.615+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.615+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.616+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.616+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.616+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.616+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.616+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.616+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.616+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.617+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.618+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.619+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.619+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.619+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.619+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.619+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.619+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.619+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.620+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.621+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.621+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.621+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.621+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.621+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.621+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.621+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.622+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.623+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.623+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.623+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.623+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.623+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.623+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.623+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.624+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.625+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.626+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.627+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.627+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.627+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.627+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.627+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.627+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.628+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.629+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.629+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.629+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.629+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.629+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.629+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.629+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.630+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.630+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.630+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.630+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.630+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.630+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.631+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.631+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.631+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.631+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.631+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.631+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.631+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.632+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.632+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.632+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.632+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.632+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.632+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.632+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.633+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.634+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.634+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.634+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.634+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.634+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.634+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.634+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.635+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.635+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.635+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.635+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.635+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.635+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.635+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.636+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.636+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.636+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.636+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.636+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.636+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.636+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.637+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.637+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.637+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.637+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.637+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.637+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.637+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.638+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.638+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.638+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.638+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.638+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.638+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.639+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.639+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.639+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.639+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.639+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.639+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.639+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.640+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.641+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.642+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.643+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.644+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.645+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.646+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.647+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.648+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.649+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.650+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.650+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.650+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.650+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.650+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.650+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.650+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.651+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.651+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.651+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.651+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.651+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.651+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.652+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.652+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.652+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.652+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.652+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.652+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.652+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.653+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.653+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.653+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.653+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.653+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.653+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.654+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.655+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.655+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.655+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.655+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.655+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.655+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.656+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.657+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.657+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.657+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.657+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.657+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.657+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.658+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.658+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.658+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.658+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.658+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.658+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.658+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.659+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.659+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.659+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.659+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.659+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.659+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.659+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.660+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.660+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.661+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.661+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.661+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.661+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.661+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.661+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.661+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.662+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.663+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.663+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.663+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.663+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.663+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.663+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.663+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.664+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.665+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.665+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.665+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.665+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.665+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.665+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.665+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.666+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.667+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.667+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.667+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.667+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.667+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.667+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.668+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.668+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.668+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.668+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.668+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.668+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.668+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.669+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.670+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.670+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.670+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.670+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.670+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.670+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.670+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.671+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.671+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.671+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.671+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.671+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.671+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.671+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.672+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.673+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.674+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.675+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.675+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.675+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.675+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.675+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.675+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.675+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.676+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.677+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.677+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.677+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.677+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.677+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.677+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.677+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.678+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.678+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.678+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.678+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.678+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.678+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.678+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.679+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.679+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.679+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.679+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.679+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.679+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.680+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.681+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.682+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.683+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.684+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.685+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.686+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.686+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.686+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.686+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.686+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.686+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.686+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.687+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.687+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.687+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.687+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.687+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.687+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.687+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.688+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.688+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.688+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.688+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.688+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.688+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.688+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.689+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.689+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.689+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.689+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.689+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.689+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.689+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.690+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.690+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.690+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.690+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.690+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.690+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.691+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.691+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.691+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.691+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.691+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.691+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.692+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.693+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.693+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.693+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.693+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.693+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.693+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.693+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.694+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.695+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.695+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.695+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.695+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.695+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.695+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.695+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.696+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.697+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.697+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.697+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.697+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.697+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.697+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.697+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.698+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.699+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.700+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.700+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.700+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.700+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.700+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.700+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.700+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.701+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.702+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.703+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.703+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.703+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.703+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.703+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.703+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.703+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.704+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.705+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.705+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.705+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.705+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.705+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.705+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.705+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.706+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.707+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.707+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.708+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.709+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.710+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.710+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.710+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.710+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.710+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.710+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.710+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.711+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.712+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.713+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.714+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.715+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.716+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.717+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.717+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.717+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.717+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.717+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.717+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.717+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.718+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.719+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.719+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.719+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.719+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.719+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.719+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.720+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.721+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.722+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.722+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.722+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.722+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.722+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.722+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.723+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.724+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.725+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.726+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.726+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.726+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.726+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.726+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.726+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.726+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.727+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.728+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.728+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.728+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.728+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.728+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.728+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.729+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.730+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.730+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.730+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.730+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.730+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.730+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.730+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.731+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.731+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.731+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.731+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.731+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.731+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.731+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.732+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.733+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.733+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.733+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.733+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.733+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.733+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.733+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.734+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.735+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.735+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.735+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.735+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.735+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.735+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.736+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.737+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.737+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.737+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.737+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.737+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.737+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.737+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.738+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.738+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.738+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.738+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.738+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.738+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.739+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.740+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.740+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.740+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.740+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.740+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.740+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.740+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.741+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.741+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.742+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.743+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.743+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.743+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.743+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.743+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.743+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.743+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.744+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.744+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.744+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.744+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.744+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.744+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.744+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.745+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.745+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.745+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.745+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.745+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.745+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.745+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.746+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.746+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.746+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.746+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.746+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.746+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.746+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.747+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.747+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.747+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.747+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.747+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.747+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.747+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.748+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.748+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.748+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.748+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.748+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.748+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.749+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.749+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.749+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.749+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.749+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.749+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.750+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.750+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.750+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.750+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.751+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.751+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.751+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.751+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.751+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.751+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.751+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.752+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.753+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.753+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.753+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.753+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.753+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.753+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.753+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.754+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.755+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.755+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.755+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.755+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.755+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.755+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.755+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.756+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.757+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.757+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.757+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.757+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.757+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.757+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.757+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.758+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.758+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.758+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.758+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.758+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.758+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.758+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.759+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.759+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.759+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.759+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.759+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.759+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.759+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.760+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.760+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.761+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.762+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.762+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.762+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.762+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.762+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.762+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.762+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.763+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.764+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.764+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.764+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.764+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.764+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.765+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.765+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.765+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.765+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.765+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.765+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.765+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.766+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.767+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.767+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.767+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.767+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.767+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.767+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.767+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.768+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.768+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.768+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.768+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.768+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.768+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.768+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.769+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.770+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.771+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.771+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.771+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.771+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.771+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.771+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.771+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.772+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.772+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.772+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.772+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.772+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.772+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.772+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.773+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.773+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.773+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.773+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.773+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.773+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.774+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.775+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.776+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.777+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.778+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.778+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.778+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.778+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.778+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.778+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.779+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.793+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.794+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.794+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.794+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.794+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.794+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.795+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.796+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.799+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.801+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.801+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.801+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.801+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.802+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.802+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.803+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.803+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.803+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.804+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.804+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.804+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.804+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.806+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.807+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.807+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.807+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.808+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.810+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.812+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.812+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.812+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.812+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.813+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.813+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.814+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.815+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.816+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.817+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.818+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.819+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.820+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.821+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.822+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.823+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.824+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.824+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.828+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.828+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.828+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.828+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.828+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.828+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.828+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.829+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.829+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.829+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.829+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.829+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.829+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.829+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.830+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.830+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.830+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.830+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.830+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.831+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.831+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.831+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.832+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.832+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.832+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.832+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.832+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.832+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.832+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.833+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.834+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.834+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.834+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.834+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.834+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.837+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.837+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.837+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.838+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.838+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.838+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.838+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.838+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.838+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.838+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.839+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.839+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.839+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.839+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.839+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.839+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.840+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.841+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.841+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.841+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.841+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.841+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.841+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.841+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.842+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.842+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.842+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.842+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.844+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.844+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.844+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.845+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.845+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.845+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.845+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.846+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.847+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.847+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.847+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.847+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.847+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.847+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.847+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.848+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.848+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.848+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.848+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.848+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.848+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.849+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.849+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.849+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.849+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.849+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.849+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.850+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.851+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.851+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.851+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.851+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.851+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.851+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.851+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.852+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.852+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.852+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.852+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.852+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.852+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.852+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.853+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.854+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.855+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.855+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.855+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.855+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.855+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.855+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.855+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.856+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.856+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.856+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.856+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.856+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.856+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.856+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.857+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.858+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.859+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.859+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.859+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.859+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.859+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.860+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.861+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.861+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.861+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.861+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.861+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.861+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.861+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.862+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.862+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.862+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.862+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.862+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.862+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.862+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.863+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.864+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.864+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.864+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.864+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.864+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.864+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.864+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.865+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.865+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.865+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.865+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.865+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.865+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.865+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.866+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.867+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.868+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.868+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.869+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.870+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.871+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.871+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.871+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.871+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.871+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.871+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.871+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.872+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.874+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.875+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.875+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.875+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.875+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.876+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.876+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.876+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.876+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.876+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.876+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.876+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.877+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.878+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.878+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.878+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.878+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.878+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.878+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.878+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.879+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.880+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.881+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.882+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.883+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.884+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.885+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.886+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.887+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.888+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.889+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.890+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.891+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.892+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.893+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.894+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.895+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.895+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.895+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.895+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.895+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.895+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.896+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.896+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.896+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.896+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.896+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.896+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.896+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.897+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.897+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.897+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.897+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.897+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.897+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.897+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.898+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.899+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.899+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.899+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.899+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.899+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.899+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.900+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.901+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.902+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.903+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.904+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.905+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.906+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.906+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.907+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.907+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.907+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.907+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.907+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.908+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://data-lake-buck/formatted/football_data/season/2024-01-29/_temporary/0/_temporary/' directory.
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO FileOutputCommitter: Saved output of task 'attempt_202401291411582144548455331391872_0008_m_000001_9' to gs://data-lake-buck/formatted/football_data/season/2024-01-29/_temporary/0/task_202401291411582144548455331391872_0008_m_000001
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO SparkHadoopMapRedUtil: attempt_202401291411582144548455331391872_0008_m_000001_9: Committed. Elapsed time: 1397 ms.
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.909+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Finished task 1.0 in stage 8.0 (TID 9). 2725 bytes result sent to driver
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 9) in 625323 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (1/2)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.910+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.911+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.912+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.913+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.914+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.915+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.916+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.917+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.918+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.919+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.920+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.921+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.922+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.923+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.924+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.924+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.925+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.925+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.925+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.925+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.925+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.925+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.925+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.926+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.926+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.926+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.926+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.926+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.926+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.926+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.927+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.927+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.927+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.927+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.927+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.927+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.927+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.928+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.929+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.930+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.931+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.932+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.933+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.934+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.935+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.936+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.937+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.938+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.939+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.940+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.941+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.942+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.943+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.943+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.943+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.943+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.943+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.944+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.945+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.946+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.947+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.948+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.949+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.950+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.951+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.952+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.953+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.954+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.955+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.956+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.957+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.958+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.959+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.960+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.960+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.961+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.961+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.961+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.961+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.961+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.961+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.962+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.962+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.962+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.962+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.962+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.963+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.963+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.963+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.963+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.963+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.963+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.964+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.965+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.966+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.967+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.968+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.969+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.970+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.971+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.972+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.973+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.974+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.975+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.976+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.977+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.978+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.979+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.980+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.980+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.980+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.980+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.980+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.980+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.981+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.981+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.981+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.981+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.981+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.982+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.983+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.984+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.985+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.986+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.987+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.988+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:23.989+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.990+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:23.991+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:23.992+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.993+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:23.994+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:23.995+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:23.996+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:23.997+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:23.998+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:23.998+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:23.998+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:23.998+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:23.998+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:23.998+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:23.999+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:23.999+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:23.999+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:23.999+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:23.999+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:23.999+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:23.999+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.000+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.001+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.002+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.003+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.004+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.005+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.006+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.007+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.008+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.009+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.010+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.011+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.012+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.013+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.014+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.015+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.016+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.017+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.017+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.017+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.017+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.017+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.017+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.017+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.018+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.019+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.020+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.021+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.022+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.023+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.024+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.025+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.026+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.027+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.028+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.029+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.030+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.031+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.032+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.032+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.032+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.032+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.032+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.032+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.032+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.033+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.033+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.033+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.033+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.033+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.033+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.034+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.035+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.036+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.037+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.038+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.039+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.039+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.039+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.039+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.040+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.041+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.042+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.043+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.044+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.045+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.046+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.047+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.048+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.049+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.051+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.052+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.053+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.054+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.055+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.056+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.057+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.058+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.059+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.060+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.061+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.062+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.063+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.064+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.065+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.066+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.067+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.067+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.067+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.067+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.067+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.067+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.067+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.068+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.068+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.069+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.070+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.071+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.072+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.073+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.073+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.074+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.075+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:23 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.076+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.077+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.078+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.079+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.080+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.081+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.082+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.083+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.084+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.085+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.086+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.087+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.088+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.089+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.090+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.091+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.092+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.093+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.094+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.095+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.096+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.097+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.098+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.099+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.100+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.101+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.102+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.103+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.104+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.106+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.106+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.107+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.108+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.109+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.110+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.111+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.112+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.113+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.114+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.115+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.116+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.117+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.118+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.119+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.120+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.121+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.122+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.123+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.124+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.125+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.126+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.127+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.128+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.129+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.130+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.131+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.132+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.133+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO Executor: Told to re-register on heartbeat
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManager: BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None) re-registering with master
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-1.europe-west9-a.c.data-lake-project-409321.internal, 33393, None)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 ERROR Inbox: Ignoring error
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.134+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.135+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.136+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.137+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.138+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.139+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.140+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 WARN Executor: Issue communicating with driver in heartbeater
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-01-29T14:22:24.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:24.142+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)
[2024-01-29T14:22:24.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - ... 3 more
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@instance-1.europe-west9-a.c.data-lake-project-409321.internal:46395
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.144+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.145+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-01-29T14:22:24.146+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-01-29T14:22:24.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-01-29T14:22:24.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-01-29T14:22:24.148+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-01-29T14:22:24.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-01-29T14:22:24.148+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-01-29T14:22:24.148+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-01-29T14:22:24.148+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - ... 8 more
[2024-01-29T14:22:24.149+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times
[2024-01-29T14:22:24.354+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO FileOutputCommitter: Saved output of task 'attempt_202401291411582144548455331391872_0008_m_000000_8' to gs://data-lake-buck/formatted/football_data/season/2024-01-29/_temporary/0/task_202401291411582144548455331391872_0008_m_000000
[2024-01-29T14:22:24.354+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO SparkHadoopMapRedUtil: attempt_202401291411582144548455331391872_0008_m_000000_8: Committed. Elapsed time: 1467 ms.
[2024-01-29T14:22:24.356+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2682 bytes result sent to driver
[2024-01-29T14:22:24.358+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 625845 ms on instance-1.europe-west9-a.c.data-lake-project-409321.internal (executor driver) (2/2)
[2024-01-29T14:22:24.358+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-01-29T14:22:24.358+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO DAGScheduler: ResultStage 8 (parquet at RawToFormatted.scala:74) finished in 625.873 s
[2024-01-29T14:22:24.359+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-01-29T14:22:24.359+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-01-29T14:22:24.362+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO DAGScheduler: Job 6 finished: parquet at RawToFormatted.scala:74, took 625.877059 s
[2024-01-29T14:22:24.363+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:24 INFO FileFormatWriter: Start to commit write Job d34ef1b0-9cec-4c00-a32e-843af4dc4f7a.
[2024-01-29T14:22:28.874+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:28 INFO FileFormatWriter: Write Job d34ef1b0-9cec-4c00-a32e-843af4dc4f7a committed. Elapsed time: 4514 ms.
[2024-01-29T14:22:28.875+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:28 INFO FileFormatWriter: Finished processing stats for write job d34ef1b0-9cec-4c00-a32e-843af4dc4f7a.
[2024-01-29T14:22:29.560+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:29 INFO InMemoryFileIndex: It took 178 ms to list leaf files for 1 paths.
[2024-01-29T14:22:29.569+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:29 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.4 KiB, free 433.4 MiB)
[2024-01-29T14:22:29.588+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.4 MiB)
[2024-01-29T14:22:29.589+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 35.0 KiB, free: 434.2 MiB)
[2024-01-29T14:22:29.591+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:29 INFO SparkContext: Created broadcast 12 from json at RawToFormatted.scala:82
[2024-01-29T14:22:31.078+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:31 INFO FileInputFormat: Total input files to process : 13
[2024-01-29T14:22:32.643+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO FileInputFormat: Total input files to process : 13
[2024-01-29T14:22:32.656+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO SparkContext: Starting job: json at RawToFormatted.scala:82
[2024-01-29T14:22:32.660+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO DAGScheduler: Got job 7 (json at RawToFormatted.scala:82) with 1 output partitions
[2024-01-29T14:22:32.660+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO DAGScheduler: Final stage: ResultStage 9 (json at RawToFormatted.scala:82)
[2024-01-29T14:22:32.660+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO DAGScheduler: Parents of final stage: List()
[2024-01-29T14:22:32.660+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO DAGScheduler: Missing parents: List()
[2024-01-29T14:22:32.660+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[31] at json at RawToFormatted.scala:82), which has no missing parents
[2024-01-29T14:22:32.664+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.2 KiB, free 433.4 MiB)
[2024-01-29T14:22:32.668+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 433.4 MiB)
[2024-01-29T14:22:32.669+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on instance-1.europe-west9-a.c.data-lake-project-409321.internal:33393 (size: 4.9 KiB, free: 434.2 MiB)
[2024-01-29T14:22:32.670+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580
[2024-01-29T14:22:32.671+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[31] at json at RawToFormatted.scala:82) (first 15 tasks are for partitions Vector(0))
[2024-01-29T14:22:32.672+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-01-29T14:22:32.673+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 10) (instance-1.europe-west9-a.c.data-lake-project-409321.internal, executor driver, partition 0, PROCESS_LOCAL, 9146 bytes)
[2024-01-29T14:22:32.674+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO Executor: Running task 0.0 in stage 9.0 (TID 10)
[2024-01-29T14:22:32.679+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:32 INFO BinaryFileRDD: Input split: Paths:/raw/football_data/competition_matches/2024-01-29/CL.json:0+200005,/raw/football_data/competition_matches/2024-01-29/EC.json:0+73922,/raw/football_data/competition_matches/2024-01-29/PD.json:0+600822,/raw/football_data/competition_matches/2024-01-29/PL.json:0+598435,/raw/football_data/competition_matches/2024-01-29/SA.json:0+591790,/raw/football_data/competition_matches/2024-01-29/WC.json:0+131839,/raw/football_data/competition_matches/2024-01-29/BL1.json:0+480554,/raw/football_data/competition_matches/2024-01-29/BSA.json:0+615137,/raw/football_data/competition_matches/2024-01-29/CLI.json:0+33016,/raw/football_data/competition_matches/2024-01-29/DED.json:0+480192,/raw/football_data/competition_matches/2024-01-29/ELC.json:0+875602,/raw/football_data/competition_matches/2024-01-29/FL1.json:0+480140,/raw/football_data/competition_matches/2024-01-29/PPL.json:0+464652
[2024-01-29T14:22:34.101+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO SparkContext: Invoking stop() from shutdown hook
[2024-01-29T14:22:34.102+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-01-29T14:22:34.131+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO SparkUI: Stopped Spark web UI at http://instance-1.europe-west9-a.c.data-lake-project-409321.internal:4040
[2024-01-29T14:22:34.138+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO DAGScheduler: Job 7 failed: json at RawToFormatted.scala:82, took 1.479220 s
[2024-01-29T14:22:34.141+0000] {spark_submit.py:571} INFO - Exception in thread "main" org.apache.spark.SparkException: Job 7 cancelled because SparkContext was shut down
[2024-01-29T14:22:34.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1248)
[2024-01-29T14:22:34.141+0000] {spark_submit.py:571} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1246)
[2024-01-29T14:22:34.142+0000] {spark_submit.py:571} INFO - at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
[2024-01-29T14:22:34.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1246)
[2024-01-29T14:22:34.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3075)
[2024-01-29T14:22:34.142+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
[2024-01-29T14:22:34.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2961)
[2024-01-29T14:22:34.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
[2024-01-29T14:22:34.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2961)
[2024-01-29T14:22:34.143+0000] {spark_submit.py:571} INFO - at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)
[2024-01-29T14:22:34.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
[2024-01-29T14:22:34.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.SparkContext.stop(SparkContext.scala:2263)
[2024-01-29T14:22:34.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.SparkContext.stop(SparkContext.scala:2216)
[2024-01-29T14:22:34.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)
[2024-01-29T14:22:34.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
[2024-01-29T14:22:34.144+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
[2024-01-29T14:22:34.145+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:34.145+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2024-01-29T14:22:34.145+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
[2024-01-29T14:22:34.145+0000] {spark_submit.py:571} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-01-29T14:22:34.145+0000] {spark_submit.py:571} INFO - at scala.util.Try$.apply(Try.scala:213)
[2024-01-29T14:22:34.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
[2024-01-29T14:22:34.146+0000] {spark_submit.py:571} INFO - at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
[2024-01-29T14:22:34.146+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[2024-01-29T14:22:34.146+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2024-01-29T14:22:34.146+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2024-01-29T14:22:34.146+0000] {spark_submit.py:571} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2024-01-29T14:22:34.146+0000] {spark_submit.py:571} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2024-01-29T14:22:34.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
[2024-01-29T14:22:34.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2024-01-29T14:22:34.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)
[2024-01-29T14:22:34.147+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:116)
[2024-01-29T14:22:34.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.$anonfun$infer$5(JsonDataSource.scala:167)
[2024-01-29T14:22:34.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2024-01-29T14:22:34.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.infer(JsonDataSource.scala:167)
[2024-01-29T14:22:34.148+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)
[2024-01-29T14:22:34.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)
[2024-01-29T14:22:34.149+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
[2024-01-29T14:22:34.149+0000] {spark_submit.py:571} INFO - at scala.Option.orElse(Option.scala:447)
[2024-01-29T14:22:34.150+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
[2024-01-29T14:22:34.150+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
[2024-01-29T14:22:34.150+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
[2024-01-29T14:22:34.150+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
[2024-01-29T14:22:34.150+0000] {spark_submit.py:571} INFO - at scala.Option.getOrElse(Option.scala:189)
[2024-01-29T14:22:34.150+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
[2024-01-29T14:22:34.151+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:362)
[2024-01-29T14:22:34.151+0000] {spark_submit.py:571} INFO - at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:341)
[2024-01-29T14:22:34.151+0000] {spark_submit.py:571} INFO - at RawToFormatted$.formattedCompetitionMatches(RawToFormatted.scala:82)
[2024-01-29T14:22:34.153+0000] {spark_submit.py:571} INFO - at RawToFormatted$.main(RawToFormatted.scala:22)
[2024-01-29T14:22:34.153+0000] {spark_submit.py:571} INFO - at RawToFormatted.main(RawToFormatted.scala)
[2024-01-29T14:22:34.153+0000] {spark_submit.py:571} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2024-01-29T14:22:34.153+0000] {spark_submit.py:571} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2024-01-29T14:22:34.154+0000] {spark_submit.py:571} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-01-29T14:22:34.154+0000] {spark_submit.py:571} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:568)
[2024-01-29T14:22:34.154+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2024-01-29T14:22:34.154+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
[2024-01-29T14:22:34.154+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
[2024-01-29T14:22:34.154+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
[2024-01-29T14:22:34.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
[2024-01-29T14:22:34.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
[2024-01-29T14:22:34.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
[2024-01-29T14:22:34.155+0000] {spark_submit.py:571} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2024-01-29T14:22:34.156+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO DAGScheduler: ResultStage 9 (json at RawToFormatted.scala:82) failed in 1.485 s due to Stage cancelled because SparkContext was shut down
[2024-01-29T14:22:34.166+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-01-29T14:22:34.202+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO MemoryStore: MemoryStore cleared
[2024-01-29T14:22:34.202+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO BlockManager: BlockManager stopped
[2024-01-29T14:22:34.209+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-01-29T14:22:34.212+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-01-29T14:22:34.221+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO SparkContext: Successfully stopped SparkContext
[2024-01-29T14:22:34.221+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO ShutdownHookManager: Shutdown hook called
[2024-01-29T14:22:34.221+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-17e43efd-3708-4676-820c-ec11a4965aab
[2024-01-29T14:22:34.225+0000] {spark_submit.py:571} INFO - 24/01/29 14:22:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-9c9f3b66-d8b5-466d-8a2d-b8e177226c70
[2024-01-29T14:22:34.355+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/juniortemgoua0/DataLake/venv/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/juniortemgoua0/DataLake/venv/lib/python3.8/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 174, in execute
    self._hook.submit(self._application)
  File "/home/juniortemgoua0/DataLake/venv/lib/python3.8/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 502, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local[*] --name arrow-spark --class RawToFormatted --queue root.default --deploy-mode client /home/juniortemgoua0/DataLake/jobs/processes/scala/spark_process/target/scala-2.12/spark_job_2.12-0.1.0.jar gs://data-lake-buck. Error code is: 56.
[2024-01-29T14:22:34.362+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=manage_data_lake_dag, task_id=job_raw_to_formatted, execution_date=20240101T000000, start_date=20240129T141115, end_date=20240129T142234
[2024-01-29T14:22:34.384+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 119 for task job_raw_to_formatted (Cannot execute: spark-submit --master local[*] --name arrow-spark --class RawToFormatted --queue root.default --deploy-mode client /home/juniortemgoua0/DataLake/jobs/processes/scala/spark_process/target/scala-2.12/spark_job_2.12-0.1.0.jar gs://data-lake-buck. Error code is: 56.; 41470)
[2024-01-29T14:22:34.397+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-01-29T14:22:34.412+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
